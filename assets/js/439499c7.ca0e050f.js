"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[587],{2637:(e,n,o)=>{o.d(n,{A:()=>t});const t=o.p+"assets/images/ai-10-db73214c9e28195999797b0c7695e150.png"},8204:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>l,frontMatter:()=>a,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"module-03-isaac/week-9-perception","title":"Week 9: Perception for Manipulation","description":"Perception for Manipulation","source":"@site/docs/module-03-isaac/week-9-perception.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-9-perception","permalink":"/hackathon-book/module-03-isaac/week-9-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-03-isaac/week-9-perception.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 9: Isaac ROS Deployment","permalink":"/hackathon-book/module-03-isaac/week-9-isaac-ros"},"next":{"title":"Week 10: Navigation with Nav2","permalink":"/hackathon-book/module-03-isaac/week-10-nav2"}}');var i=o(4848),s=o(8453);const a={},r="Week 9: Perception for Manipulation",c={},p=[{value:"The Manipulation Perception Challenge",id:"the-manipulation-perception-challenge",level:2},{value:"Object Detection vs. Pose Estimation",id:"object-detection-vs-pose-estimation",level:2},{value:"2D Object Detection",id:"2d-object-detection",level:3},{value:"6-DOF Pose Estimation",id:"6-dof-pose-estimation",level:3},{value:"DOPE: Deep Object Pose Estimation",id:"dope-deep-object-pose-estimation",level:2},{value:"DOPE Architecture",id:"dope-architecture",level:3},{value:"Running Isaac ROS DOPE",id:"running-isaac-ros-dope",level:3},{value:"Training DOPE on Custom Objects",id:"training-dope-on-custom-objects",level:2},{value:"Step 1: Generate Synthetic Training Data",id:"step-1-generate-synthetic-training-data",level:3},{value:"Step 2: Train DOPE Model",id:"step-2-train-dope-model",level:3},{value:"Depth Perception for Manipulation",id:"depth-perception-for-manipulation",level:2},{value:"Integrating Depth with DOPE",id:"integrating-depth-with-dope",level:3},{value:"Grasp Planning with 6-DOF Poses",id:"grasp-planning-with-6-dof-poses",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-9-perception-for-manipulation",children:"Week 9: Perception for Manipulation"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Perception for Manipulation",src:o(2637).A+"",width:"736",height:"1104"})}),"\n",(0,i.jsx)(n.h2,{id:"the-manipulation-perception-challenge",children:"The Manipulation Perception Challenge"}),"\n",(0,i.jsxs)(n.p,{children:["Robotic manipulation requires knowing the ",(0,i.jsx)(n.strong,{children:"6-DOF pose"})," (3D position + 3D orientation) of objects with millimeter-level accuracy. A humanoid robot grasping a cup must know:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Position (x, y, z)"}),": Where is the cup's center?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Orientation (roll, pitch, yaw)"}),": How is the cup rotated?"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Traditional 2D object detection only provides bounding boxes, insufficient for 3D grasping."}),"\n",(0,i.jsx)(n.h2,{id:"object-detection-vs-pose-estimation",children:"Object Detection vs. Pose Estimation"}),"\n",(0,i.jsx)(n.h3,{id:"2d-object-detection",children:"2D Object Detection"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": [x, y, width, height, class, confidence]"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),': "Is there a cup in the image?"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitation"}),": No depth or orientation information"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"6-dof-pose-estimation",children:"6-DOF Pose Estimation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": [x, y, z, qw, qx, qy, qz, class, confidence]"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),': "Where exactly is the cup in 3D space?"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enables"}),": Grasp planning, collision avoidance, precise placement"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"dope-deep-object-pose-estimation",children:"DOPE: Deep Object Pose Estimation"}),"\n",(0,i.jsx)(n.p,{children:"DOPE (Deep Object Pose Estimation) is a CNN that predicts 6-DOF poses from RGB images. Trained on synthetic Isaac Sim data, DOPE achieves real-world accuracy through domain randomization."}),"\n",(0,i.jsx)(n.h3,{id:"dope-architecture",children:"DOPE Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Conceptual DOPE pipeline\ndef dope_inference(rgb_image):\n    # 1. Feature extraction (ResNet backbone)\n    features = resnet50(rgb_image)\n\n    # 2. Belief maps for 3D keypoints (e.g., object corners)\n    belief_maps = conv2d(features, num_keypoints=8)\n\n    # 3. PnP algorithm to recover 6-DOF pose from 2D keypoints\n    keypoints_2d = find_peaks(belief_maps)\n    pose_6dof = solve_pnp(keypoints_2d, keypoints_3d_model, camera_matrix)\n\n    return pose_6dof  # [x, y, z, quaternion]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-isaac-ros-dope",children:"Running Isaac ROS DOPE"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Launch file for DOPE object pose estimation\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    # Path to trained DOPE model for specific object\n    model_path = LaunchConfiguration('model_path', default='/workspaces/isaac_ros-dev/models/dope_soup_60.onnx')\n\n    return LaunchDescription([\n        # Camera input\n        Node(\n            package='realsense2_camera',\n            executable='realsense2_camera_node',\n            parameters=[{\n                'rgb_camera.profile': '640x480x30',\n                'depth_module.profile': '640x480x30',\n                'enable_depth': True,\n                'align_depth.enable': True  # Align depth to RGB\n            }]\n        ),\n\n        # DOPE inference node (TensorRT-accelerated)\n        Node(\n            package='isaac_ros_dope',\n            executable='dope_decoder',\n            name='dope',\n            parameters=[{\n                'object_name': 'soup_can',      # Object class this model detects\n                'model_file_path': model_path,\n                'configuration_file': '/workspaces/isaac_ros-dev/config/dope_config.yaml',\n                # Object dimensions in meters (for PnP)\n                'object_dimensions': [0.067, 0.067, 0.103],  # [width, depth, height]\n                # Detection threshold\n                'map_peak_threshold': 0.1\n            }],\n            remappings=[\n                ('image', '/camera/color/image_raw'),\n                ('camera_info', '/camera/color/camera_info'),\n                ('dope/pose_array', '/poses')  # Output: PoseArray message\n            ]\n        ),\n\n        # TF broadcaster to publish object pose in robot frame\n        Node(\n            package='isaac_ros_dope',\n            executable='dope_pose_to_tf',\n            parameters=[{\n                'object_frame_id': 'soup_can',\n                'reference_frame_id': 'camera_color_optical_frame'\n            }],\n            remappings=[\n                ('dope/pose_array', '/poses')\n            ]\n        ),\n\n        # Visualization\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            arguments=['-d', './config/dope_visualization.rviz']\n        )\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"training-dope-on-custom-objects",children:"Training DOPE on Custom Objects"}),"\n",(0,i.jsx)(n.p,{children:"To detect your own objects, train DOPE using synthetic data from Isaac Sim:"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-generate-synthetic-training-data",children:"Step 1: Generate Synthetic Training Data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\nfrom omni.isaac.kit import SimulationApp\n\nsimulation_app = SimulationApp({"headless": True})\n\n# Load your custom object (e.g., custom tool)\ncustom_object = rep.create.from_usd(\n    "./assets/custom_tool.usd",\n    semantics=[("class", "custom_tool")]\n)\n\ndef randomize_scene():\n    # Randomize object pose\n    with custom_object:\n        rep.modify.pose(\n            position=rep.distribution.uniform((-0.3, -0.3, 0.5), (0.3, 0.3, 0.8)),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n        # Randomize appearance\n        rep.randomizer.color(\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\n        )\n\n    # Randomize lighting\n    light = rep.create.light(\n        light_type="Dome",\n        intensity=rep.distribution.uniform(500, 2000)\n    )\n\n    return custom_object\n\nrep.randomizer.register(randomize_scene)\n\n# Camera for data capture\ncamera = rep.create.camera(\n    position=(1.0, 1.0, 0.6),\n    look_at=(0, 0, 0.5)\n)\n\n# DOPE requires RGB + keypoint annotations\nwriter = rep.WriterRegistry.get("BasicWriter")\nwriter.initialize(\n    output_dir="./dope_training_data",\n    rgb=True,\n    bounding_box_2d_tight=True,\n    bounding_box_3d=True  # 3D keypoints for DOPE training\n)\nwriter.attach([camera])\n\n# Generate 10,000 training images\nwith rep.trigger.on_frame(num_frames=10000):\n    rep.randomizer.randomize_scene()\n\nrep.orchestrator.run()\nsimulation_app.close()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-train-dope-model",children:"Step 2: Train DOPE Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Clone DOPE training repository\ngit clone https://github.com/NVlabs/Deep_Object_Pose.git\ncd Deep_Object_Pose\n\n# Prepare dataset (convert Isaac Sim output to DOPE format)\npython scripts/prepare_isaac_sim_data.py \\\n    --input_dir ./dope_training_data \\\n    --output_dir ./dope_formatted\n\n# Train DOPE network\npython train.py \\\n    --data ./dope_formatted \\\n    --object custom_tool \\\n    --epochs 60 \\\n    --batch_size 32 \\\n    --gpus 1\n\n# Export to ONNX for TensorRT optimization\npython export_onnx.py \\\n    --checkpoint ./checkpoints/custom_tool_epoch60.pth \\\n    --output custom_tool.onnx\n"})}),"\n",(0,i.jsx)(n.h2,{id:"depth-perception-for-manipulation",children:"Depth Perception for Manipulation"}),"\n",(0,i.jsx)(n.p,{children:"Depth sensors provide distance measurements critical for grasp planning."}),"\n",(0,i.jsx)(n.h3,{id:"integrating-depth-with-dope",children:"Integrating Depth with DOPE"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseArray\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass DepthEnhancedPose(Node):\n    def __init__(self):\n        super().__init__('depth_enhanced_pose')\n\n        # Subscribe to DOPE poses\n        self.pose_sub = self.create_subscription(\n            PoseArray,\n            '/poses',\n            self.pose_callback,\n            10\n        )\n\n        # Subscribe to depth image\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/aligned_depth_to_color/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.bridge = CvBridge()\n        self.latest_depth = None\n\n    def depth_callback(self, msg):\n        # Convert ROS depth image to numpy array\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n\n    def pose_callback(self, msg):\n        if self.latest_depth is None:\n            return\n\n        for pose in msg.poses:\n            # DOPE provides pose estimate, but depth can refine Z coordinate\n            # Project pose to image coordinates\n            u, v = self.project_to_image(pose.position)\n\n            # Look up depth at that pixel\n            if 0 <= u < self.latest_depth.shape[1] and 0 <= v < self.latest_depth.shape[0]:\n                measured_depth = self.latest_depth[v, u]\n\n                # Refine pose Z coordinate using depth sensor\n                refined_z = measured_depth * 0.001  # Convert mm to meters\n\n                self.get_logger().info(\n                    f\"DOPE Z: {pose.position.z:.3f}m, \"\n                    f\"Depth Z: {refined_z:.3f}m, \"\n                    f\"Error: {abs(pose.position.z - refined_z)*1000:.1f}mm\"\n                )\n\n    def project_to_image(self, position):\n        # Camera intrinsics (replace with actual values from camera_info)\n        fx, fy = 615.0, 615.0  # Focal lengths\n        cx, cy = 320.0, 240.0  # Principal point\n\n        # Project 3D point to 2D image\n        u = int(fx * position.x / position.z + cx)\n        v = int(fy * position.y / position.z + cy)\n        return u, v\n\ndef main():\n    rclpy.init()\n    node = DepthEnhancedPose()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"grasp-planning-with-6-dof-poses",children:"Grasp Planning with 6-DOF Poses"}),"\n",(0,i.jsx)(n.p,{children:"Once you have object poses, compute grasp configurations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.spatial.transform import Rotation\n\ndef plan_top_down_grasp(object_pose):\n    """\n    Plan a simple top-down grasp for a known object.\n\n    Args:\n        object_pose: geometry_msgs/Pose of the object\n\n    Returns:\n        grasp_pose: Desired end-effector pose for grasping\n    """\n    # Extract object position\n    obj_pos = np.array([\n        object_pose.position.x,\n        object_pose.position.y,\n        object_pose.position.z\n    ])\n\n    # Grasp approach: 10cm above object, gripper pointing down\n    grasp_offset = np.array([0, 0, 0.10])  # 10cm above\n    grasp_pos = obj_pos + grasp_offset\n\n    # Gripper orientation: Z-axis pointing down\n    grasp_rot = Rotation.from_euler(\'xyz\', [180, 0, 0], degrees=True)\n\n    # Align gripper with object orientation (for non-symmetric objects)\n    obj_rot = Rotation.from_quat([\n        object_pose.orientation.x,\n        object_pose.orientation.y,\n        object_pose.orientation.z,\n        object_pose.orientation.w\n    ])\n\n    # Combined rotation\n    final_rot = obj_rot * grasp_rot\n\n    return grasp_pos, final_rot.as_quat()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"DOPE Deployment"}),": Run Isaac ROS DOPE with a pre-trained model (soup can or cracker box) and visualize detections in RViz."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Depth Fusion"}),": Implement the DepthEnhancedPose node and compare DOPE Z estimates with depth sensor measurements."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Custom Object Detection"}),": Choose a household object, generate 1000 synthetic training images in Isaac Sim, and train a DOPE model."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Grasp Planning"}),": Given a detected object pose, compute a grasp pose and publish it as a TF frame."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"/hackathon-book/module-03-isaac/week-10-nav2",children:"Week 10 - Navigation with Nav2"})]})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var t=o(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);