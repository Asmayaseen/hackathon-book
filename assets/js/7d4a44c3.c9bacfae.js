"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[5849],{7419:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/ai-16-9e7d952177784162a04ea56d55dcfdb5.png"},8343:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-04-vla/week-13-capstone-implementation","title":"Week 13: Capstone Implementation - Complete System Integration","description":"Capstone Implementation","source":"@site/docs/module-04-vla/week-13-capstone-implementation.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-13-capstone-implementation","permalink":"/hackathon-book/module-04-vla/week-13-capstone-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-04-vla/week-13-capstone-implementation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI","permalink":"/hackathon-book/module-04-vla/week-13-capstone-intro"}}');var a=t(4848),s=t(8453);const i={},r="Week 13: Capstone Implementation - Complete System Integration",l={},c=[{value:"Step-by-Step Integration Guide",id:"step-by-step-integration-guide",level:2},{value:"Part 1: Voice-to-Intent Pipeline",id:"part-1-voice-to-intent-pipeline",level:2},{value:"Wake-Word Detection and Command Capture",id:"wake-word-detection-and-command-capture",level:3},{value:"Part 2: LLM Planning Integration",id:"part-2-llm-planning-integration",level:2},{value:"ReAct Task Executor with ROS 2",id:"react-task-executor-with-ros-2",level:3},{value:"Part 3: Text-to-Speech Feedback Node",id:"part-3-text-to-speech-feedback-node",level:2},{value:"Part 4: Launch File for Complete System",id:"part-4-launch-file-for-complete-system",level:2},{value:"Running the Complete System",id:"running-the-complete-system",level:2},{value:"Testing the System",id:"testing-the-system",level:2},{value:"Test 1: Simple Fetch Task",id:"test-1-simple-fetch-task",level:3},{value:"Test 2: Multi-Step Task",id:"test-2-multi-step-task",level:3},{value:"Test 3: Adaptive Replanning",id:"test-3-adaptive-replanning",level:3},{value:"Optimization and Debugging",id:"optimization-and-debugging",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-13-capstone-implementation---complete-system-integration",children:"Week 13: Capstone Implementation - Complete System Integration"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Capstone Implementation",src:t(7419).A+"",width:"736",height:"736"})}),"\n",(0,a.jsx)(n.h2,{id:"step-by-step-integration-guide",children:"Step-by-Step Integration Guide"}),"\n",(0,a.jsx)(n.p,{children:"This chapter provides a complete implementation of the VLA capstone system, integrating voice recognition, LLM planning, visual grounding, navigation, manipulation, and natural language feedback. Each component builds on previous modules to create a fully autonomous humanoid assistant."}),"\n",(0,a.jsx)(n.h2,{id:"part-1-voice-to-intent-pipeline",children:"Part 1: Voice-to-Intent Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"wake-word-detection-and-command-capture",children:"Wake-Word Detection and Command Capture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nimport threading\nimport queue\n\nclass VoiceCommandNode(Node):\n    """\n    ROS 2 node for continuous voice command recognition.\n    Implements wake-word detection and Whisper transcription.\n    """\n\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n\n        # Publisher for transcribed commands\n        self.command_pub = self.create_publisher(String, \'/voice_commands\', 10)\n\n        # Load Whisper model for speech recognition\n        self.get_logger().info("Loading Whisper model...")\n        self.whisper_model = whisper.load_model("base")\n\n        # Audio configuration\n        self.sample_rate = 16000\n        self.wake_word = "hey robot"\n        self.listening = False\n\n        # Audio queue for background recording\n        self.audio_queue = queue.Queue()\n\n        # Start audio capture thread\n        self.audio_thread = threading.Thread(target=self._audio_capture_loop, daemon=True)\n        self.audio_thread.start()\n\n        self.get_logger().info("Voice command system ready. Say \'Hey Robot\' to activate.")\n\n    def _audio_capture_loop(self):\n        """\n        Continuously capture audio and process for wake-word detection.\n        Runs in background thread to avoid blocking ROS callbacks.\n        """\n        while True:\n            # Record 3-second chunks\n            audio = sd.rec(\n                int(3 * self.sample_rate),\n                samplerate=self.sample_rate,\n                channels=1,\n                dtype=\'float32\'\n            )\n            sd.wait()\n\n            # Transcribe audio\n            audio_flat = audio.flatten()\n            result = self.whisper_model.transcribe(\n                audio_flat,\n                language=\'en\',\n                fp16=False  # CPU compatibility\n            )\n\n            transcription = result[\'text\'].lower().strip()\n            self.get_logger().debug(f"Heard: {transcription}")\n\n            # Check for wake-word\n            if self.wake_word in transcription:\n                self.get_logger().info("Wake-word detected! Listening for command...")\n                self._capture_command()\n\n    def _capture_command(self):\n        """\n        Record longer audio segment for command after wake-word.\n        """\n        # Record 5-second command\n        self.get_logger().info("Recording command...")\n        audio = sd.rec(\n            int(5 * self.sample_rate),\n            samplerate=self.sample_rate,\n            channels=1,\n            dtype=\'float32\'\n        )\n        sd.wait()\n\n        # Transcribe command\n        audio_flat = audio.flatten()\n        result = self.whisper_model.transcribe(audio_flat, language=\'en\', fp16=False)\n\n        command = result[\'text\'].strip()\n        self.get_logger().info(f"Command received: {command}")\n\n        # Publish command to planning system\n        msg = String()\n        msg.data = command\n        self.command_pub.publish(msg)\n\ndef main():\n    rclpy.init()\n    node = VoiceCommandNode()\n    rclpy.spin(node)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"part-2-llm-planning-integration",children:"Part 2: LLM Planning Integration"}),"\n",(0,a.jsx)(n.h3,{id:"react-task-executor-with-ros-2",children:"ReAct Task Executor with ROS 2"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nimport os\nimport json\nfrom typing import Dict, List, Tuple\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\n\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\nclass VLAExecutorNode(Node):\n    """\n    Main execution node integrating LLM planning with robot actions.\n    Implements ReAct pattern for adaptive task execution.\n    """\n\n    def __init__(self):\n        super().__init__(\'vla_executor\')\n\n        # Subscribe to voice commands\n        self.cmd_sub = self.create_subscription(\n            String,\n            \'/voice_commands\',\n            self.command_callback,\n            10\n        )\n\n        # Subscribe to camera feed for visual grounding\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Publishers for robot control\n        self.nav_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.grasp_pub = self.create_publisher(String, \'/grasp_command\', 10)\n        self.speech_pub = self.create_publisher(String, \'/tts_output\', 10)\n\n        # State tracking\n        self.current_image = None\n        self.bridge = CvBridge()\n        self.world_state = {\n            "robot_location": "living_room",\n            "held_object": None,\n            "known_objects": {}\n        }\n\n        self.execution_history = []\n        self.get_logger().info("VLA Executor ready.")\n\n    def image_callback(self, msg):\n        """Store latest camera image for visual grounding."""\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n\n    def command_callback(self, msg):\n        """\n        Receive voice command and execute using ReAct loop.\n\n        Args:\n            msg: String message containing natural language command\n        """\n        command = msg.data\n        self.get_logger().info(f"Executing command: {command}")\n\n        # Generate initial plan with GPT-4\n        plan = self._generate_plan(command)\n\n        # Execute plan with ReAct loop\n        success = self._execute_react_loop(command, plan)\n\n        if success:\n            self._speak("Task completed successfully!")\n        else:\n            self._speak("I encountered a problem and couldn\'t complete the task.")\n\n    def _generate_plan(self, goal: str) -> List[Dict]:\n        """\n        Use GPT-4 to decompose goal into action sequence.\n\n        Returns:\n            List of action dictionaries with \'type\', \'parameters\'\n        """\n        system_prompt = """You are a task planner for a humanoid robot.\n        Generate step-by-step plans using these actions:\n\n        - navigate(location): Move to location\n        - locate(object_description): Find object using camera\n        - approach(object): Move close to object for manipulation\n        - grasp(object): Pick up object\n        - place(object, location): Put down object\n        - check(condition): Verify environment state\n\n        Return JSON array: [{"action": "type", "params": {...}}, ...]\n        """\n\n        user_prompt = f"""Goal: {goal}\n\n        Current state:\n        - Robot location: {self.world_state[\'robot_location\']}\n        - Held object: {self.world_state[\'held_object\']}\n\n        Generate action plan:\n        """\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.2\n        )\n\n        # Parse JSON plan\n        plan_text = response.choices[0].message.content\n        try:\n            # Extract JSON from response\n            start = plan_text.find(\'[\')\n            end = plan_text.rfind(\']\') + 1\n            plan = json.loads(plan_text[start:end])\n            return plan\n        except json.JSONDecodeError:\n            self.get_logger().error("Failed to parse plan from LLM")\n            return []\n\n    def _execute_react_loop(self, goal: str, initial_plan: List[Dict]) -> bool:\n        """\n        Execute plan with adaptive replanning based on observations.\n\n        Args:\n            goal: Original user goal\n            initial_plan: Initial action sequence from LLM\n\n        Returns:\n            bool: True if goal achieved\n        """\n        plan = initial_plan\n        step = 0\n        max_steps = 20\n\n        while step < max_steps:\n            if not plan:\n                self.get_logger().info("Plan complete!")\n                return True\n\n            # Get next action\n            action = plan[0]\n            self.get_logger().info(f"Step {step}: {action}")\n\n            # Execute action and get observation\n            success, observation = self._execute_action(action)\n\n            # Update execution history\n            self.execution_history.append({\n                "step": step,\n                "action": action,\n                "success": success,\n                "observation": observation\n            })\n\n            if success:\n                # Remove completed action from plan\n                plan = plan[1:]\n                self._speak(f"Completed: {action[\'action\']}")\n            else:\n                # Replan on failure\n                self.get_logger().warn(f"Action failed: {observation}")\n                self._speak(f"Hmm, {observation}. Let me try a different approach.")\n\n                # Generate new plan from current state\n                plan = self._replan(goal, observation)\n\n                if not plan:\n                    self.get_logger().error("Unable to replan. Task failed.")\n                    return False\n\n            step += 1\n\n        self.get_logger().warn("Max steps reached without completion.")\n        return False\n\n    def _execute_action(self, action: Dict) -> Tuple[bool, str]:\n        """\n        Execute single action and return result.\n\n        Returns:\n            (success, observation): Execution result and observation\n        """\n        action_type = action[\'action\']\n        params = action.get(\'params\', {})\n\n        if action_type == \'navigate\':\n            return self._navigate(params[\'location\'])\n\n        elif action_type == \'locate\':\n            return self._locate_object(params[\'object_description\'])\n\n        elif action_type == \'approach\':\n            return self._approach_object(params[\'object\'])\n\n        elif action_type == \'grasp\':\n            return self._grasp_object(params[\'object\'])\n\n        elif action_type == \'place\':\n            return self._place_object(params[\'location\'])\n\n        elif action_type == \'check\':\n            return self._check_condition(params[\'condition\'])\n\n        else:\n            return False, f"Unknown action type: {action_type}"\n\n    def _navigate(self, location: str) -> Tuple[bool, str]:\n        """Navigate to location using Nav2."""\n        self.get_logger().info(f"Navigating to {location}...")\n\n        # In real implementation: publish nav goal and wait for result\n        # Mock success for demonstration\n        self.world_state[\'robot_location\'] = location\n        return True, f"Arrived at {location}"\n\n    def _locate_object(self, description: str) -> Tuple[bool, str]:\n        """\n        Use CLIP to find object in camera view.\n\n        Returns:\n            (success, observation): Whether found and location description\n        """\n        if self.current_image is None:\n            return False, "No camera image available"\n\n        # Use CLIP visual grounding (from Week 12)\n        # Simplified for demonstration\n        from PIL import Image as PILImage\n        import clip\n        import torch\n\n        device = "cuda" if torch.cuda.is_available() else "cpu"\n        model, preprocess = clip.load("ViT-B/32", device=device)\n\n        # Convert image\n        pil_image = PILImage.fromarray(self.current_image)\n        image_input = preprocess(pil_image).unsqueeze(0).to(device)\n\n        # Search for object\n        text_input = clip.tokenize([description, "empty scene"]).to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image_input)\n            text_features = model.encode_text(text_input)\n\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            similarity = (image_features @ text_features.T).squeeze(0)\n            probs = similarity.softmax(dim=0)\n\n        # Check if object detected with confidence\n        if probs[0] > 0.7:\n            # Store in world state\n            self.world_state[\'known_objects\'][description] = {\n                "location": self.world_state[\'robot_location\'],\n                "confidence": float(probs[0])\n            }\n            return True, f"Found {description} with confidence {probs[0]:.2f}"\n        else:\n            return False, f"Could not find {description} in view"\n\n    def _grasp_object(self, object_name: str) -> Tuple[bool, str]:\n        """Execute grasp using MoveIt2."""\n        self.get_logger().info(f"Grasping {object_name}...")\n\n        # Publish grasp command to manipulation controller\n        msg = String()\n        msg.data = f"grasp:{object_name}"\n        self.grasp_pub.publish(msg)\n\n        # Mock success\n        self.world_state[\'held_object\'] = object_name\n        return True, f"Successfully grasped {object_name}"\n\n    def _place_object(self, location: str) -> Tuple[bool, str]:\n        """Place held object at location."""\n        if self.world_state[\'held_object\'] is None:\n            return False, "Not holding any object"\n\n        object_name = self.world_state[\'held_object\']\n        self.get_logger().info(f"Placing {object_name} at {location}...")\n\n        # Publish place command\n        msg = String()\n        msg.data = f"place:{location}"\n        self.grasp_pub.publish(msg)\n\n        self.world_state[\'held_object\'] = None\n        return True, f"Placed {object_name} at {location}"\n\n    def _approach_object(self, object_name: str) -> Tuple[bool, str]:\n        """Move close to object for manipulation."""\n        if object_name not in self.world_state[\'known_objects\']:\n            return False, f"Location of {object_name} unknown"\n\n        # Navigate to object location\n        obj_info = self.world_state[\'known_objects\'][object_name]\n        return True, f"Approached {object_name}"\n\n    def _check_condition(self, condition: str) -> Tuple[bool, str]:\n        """Verify environmental condition using vision/sensors."""\n        # Use GPT-4V for visual question answering\n        # Simplified for demonstration\n        return True, f"Checked: {condition}"\n\n    def _replan(self, goal: str, failure_reason: str) -> List[Dict]:\n        """\n        Generate new plan after failure using failure observation.\n        """\n        prompt = f"""Original goal: {goal}\n\n        Previous plan failed because: {failure_reason}\n\n        Execution history:\n        {json.dumps(self.execution_history[-3:], indent=2)}\n\n        Current state:\n        {json.dumps(self.world_state, indent=2)}\n\n        Generate alternative plan as JSON array:\n        """\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.3\n        )\n\n        # Parse new plan\n        plan_text = response.choices[0].message.content\n        try:\n            start = plan_text.find(\'[\')\n            end = plan_text.rfind(\']\') + 1\n            new_plan = json.loads(plan_text[start:end])\n            return new_plan\n        except:\n            return []\n\n    def _speak(self, text: str):\n        """\n        Generate speech output via TTS.\n        """\n        self.get_logger().info(f"Speaking: {text}")\n\n        # Publish to TTS node\n        msg = String()\n        msg.data = text\n        self.speech_pub.publish(msg)\n\ndef main():\n    rclpy.init()\n    node = VLAExecutorNode()\n    rclpy.spin(node)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"part-3-text-to-speech-feedback-node",children:"Part 3: Text-to-Speech Feedback Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pyttsx3\n\nclass TTSNode(Node):\n    """\n    Text-to-Speech node for natural language feedback.\n    """\n\n    def __init__(self):\n        super().__init__(\'tts_node\')\n\n        # Subscribe to speech requests\n        self.subscription = self.create_subscription(\n            String,\n            \'/tts_output\',\n            self.speak_callback,\n            10\n        )\n\n        # Initialize TTS engine\n        self.engine = pyttsx3.init()\n        self.engine.setProperty(\'rate\', 150)  # Words per minute\n        self.engine.setProperty(\'volume\', 0.9)\n\n        self.get_logger().info("TTS system ready.")\n\n    def speak_callback(self, msg):\n        """Convert text to speech."""\n        text = msg.data\n        self.get_logger().info(f"Speaking: {text}")\n\n        # Synthesize speech (non-blocking)\n        self.engine.say(text)\n        self.engine.runAndWait()\n\ndef main():\n    rclpy.init()\n    node = TTSNode()\n    rclpy.spin(node)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"part-4-launch-file-for-complete-system",children:"Part 4: Launch File for Complete System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# launch/vla_capstone.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Voice command node\n        Node(\n            package='vla_capstone',\n            executable='voice_command_node',\n            name='voice_command',\n            output='screen'\n        ),\n\n        # VLA executor (planning + vision + control)\n        Node(\n            package='vla_capstone',\n            executable='vla_executor_node',\n            name='vla_executor',\n            output='screen',\n            parameters=[{\n                'use_sim_time': False\n            }]\n        ),\n\n        # Text-to-speech feedback\n        Node(\n            package='vla_capstone',\n            executable='tts_node',\n            name='tts',\n            output='screen'\n        ),\n\n        # Camera driver (replace with actual camera node)\n        Node(\n            package='usb_cam',\n            executable='usb_cam_node_exe',\n            name='camera',\n            parameters=[{\n                'video_device': '/dev/video0',\n                'framerate': 30.0,\n                'image_width': 640,\n                'image_height': 480\n            }]\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"running-the-complete-system",children:"Running the Complete System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch core robot systems (simulation or hardware)\nros2 launch your_robot_description robot.launch.py\n\n# Terminal 2: Launch VLA capstone system\nros2 launch vla_capstone vla_capstone.launch.py\n\n# Terminal 3: Monitor execution\nros2 topic echo /voice_commands\nros2 topic echo /tts_output\n\n# Terminal 4: Visualization\nrviz2 -d vla_capstone.rviz\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-the-system",children:"Testing the System"}),"\n",(0,a.jsx)(n.h3,{id:"test-1-simple-fetch-task",children:"Test 1: Simple Fetch Task"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Command"}),': "Hey robot, bring me the red mug from the table."']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:'System confirms: "Navigating to table..."'}),"\n",(0,a.jsx)(n.li,{children:"Locates red mug using CLIP"}),"\n",(0,a.jsx)(n.li,{children:"Approaches and grasps mug"}),"\n",(0,a.jsx)(n.li,{children:'Returns: "Here is your red mug."'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"test-2-multi-step-task",children:"Test 2: Multi-Step Task"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Command"}),': "Clear the desk by moving all mugs to the dish rack."']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Navigates to desk"}),"\n",(0,a.jsx)(n.li,{children:"Iteratively locates mugs"}),"\n",(0,a.jsx)(n.li,{children:"For each mug: grasp \u2192 navigate to dish rack \u2192 place"}),"\n",(0,a.jsx)(n.li,{children:'Confirms: "Desk cleared. Moved 3 mugs to dish rack."'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"test-3-adaptive-replanning",children:"Test 3: Adaptive Replanning"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Command"}),': "Get the laptop from the bedroom."']}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Navigates to bedroom"}),"\n",(0,a.jsx)(n.li,{children:'If laptop not found: "I don\'t see a laptop in the bedroom. Should I check another room?"'}),"\n",(0,a.jsx)(n.li,{children:'User: "Try the living room."'}),"\n",(0,a.jsx)(n.li,{children:"Replans and searches living room"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"optimization-and-debugging",children:"Optimization and Debugging"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Performance Tuning"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Use ",(0,a.jsx)(n.code,{children:"gpt-3.5-turbo"})," for faster planning (3x speedup over GPT-4)"]}),"\n",(0,a.jsx)(n.li,{children:"Cache CLIP embeddings for known objects"}),"\n",(0,a.jsx)(n.li,{children:"Run perception in separate thread to avoid blocking"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Common Issues"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Whisper latency"}),": Use ",(0,a.jsx)(n.code,{children:"tiny"})," or ",(0,a.jsx)(n.code,{children:"base"})," model; consider distil-whisper"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CLIP false positives"}),": Increase confidence threshold to 0.8+"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Plan failures"}),": Add more few-shot examples to system prompt"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TTS blocking"}),": Run in separate process or use async synthesis"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"You have now built a complete Vision-Language-Action system integrating speech recognition, LLM planning, visual grounding, and robot control. This represents the current state-of-the-art in embodied AI, demonstrating how foundation models (GPT-4, CLIP, Whisper) can be orchestrated for real-world robotic applications."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Next Steps"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Deploy on physical humanoid platform"}),"\n",(0,a.jsx)(n.li,{children:"Add more sophisticated error recovery (human-in-the-loop requests)"}),"\n",(0,a.jsx)(n.li,{children:"Implement memory persistence (remember object locations across sessions)"}),"\n",(0,a.jsx)(n.li,{children:"Integrate advanced manipulation (dexterous grasping, tool use)"}),"\n",(0,a.jsx)(n.li,{children:"Extend to multi-robot collaboration"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Congratulations on completing Module 4! You are now equipped to build next-generation autonomous humanoid robots with conversational AI capabilities."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);