"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[3096],{8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var a=r(6540);const s={},i=a.createContext(s);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:n},e.children)}},9730:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-03-isaac/week-9-isaac-ros","title":"Week 9: Isaac ROS Deployment","description":"Introduction to Isaac ROS","source":"@site/docs/module-03-isaac/week-9-isaac-ros.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-9-isaac-ros","permalink":"/hackathon-book/module-03-isaac/week-9-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-03-isaac/week-9-isaac-ros.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 8: Synthetic Data Generation","permalink":"/hackathon-book/module-03-isaac/week-8-synthetic-data"},"next":{"title":"Week 9: Perception for Manipulation","permalink":"/hackathon-book/module-03-isaac/week-9-perception"}}');var s=r(4848),i=r(8453);const t={},o="Week 9: Isaac ROS Deployment",c={},l=[{value:"Introduction to Isaac ROS",id:"introduction-to-isaac-ros",level:2},{value:"Why Hardware Acceleration Matters",id:"why-hardware-acceleration-matters",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:2},{value:"GEM (GPU-accelerated Module) Structure",id:"gem-gpu-accelerated-module-structure",level:3},{value:"Installation on Jetson Orin",id:"installation-on-jetson-orin",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installation Steps",id:"installation-steps",level:3},{value:"Hardware-Accelerated Visual SLAM",id:"hardware-accelerated-visual-slam",level:2},{value:"Traditional CPU VSLAM",id:"traditional-cpu-vslam",level:3},{value:"Isaac ROS VSLAM",id:"isaac-ros-vslam",level:3},{value:"Running Isaac ROS VSLAM",id:"running-isaac-ros-vslam",level:3},{value:"Stereo Vision for Depth Perception",id:"stereo-vision-for-depth-perception",level:2},{value:"Isaac ROS Stereo Disparity",id:"isaac-ros-stereo-disparity",level:3},{value:"DNN Inference with TensorRT",id:"dnn-inference-with-tensorrt",level:2},{value:"Converting PyTorch Model to TensorRT",id:"converting-pytorch-model-to-tensorrt",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-9-isaac-ros-deployment",children:"Week 9: Isaac ROS Deployment"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-isaac-ros",children:"Introduction to Isaac ROS"}),"\n",(0,s.jsxs)(n.p,{children:["Isaac ROS is a collection of ",(0,s.jsx)(n.strong,{children:"hardware-accelerated ROS 2 packages"})," (called GEMs) designed to run perception and navigation algorithms on NVIDIA GPUs. These packages leverage CUDA and TensorRT to achieve 10-100x speedups over CPU-based implementations."]}),"\n",(0,s.jsx)(n.h2,{id:"why-hardware-acceleration-matters",children:"Why Hardware Acceleration Matters"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots require real-time perception for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Balance control"}),": Processing IMU and vision data at 200+ Hz"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Obstacle avoidance"}),": Detecting objects in < 50ms for dynamic environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation"}),": 6-DOF pose estimation at 30 Hz for grasping moving objects"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"CPU-based perception cannot meet these latency requirements. GPU acceleration makes real-time Physical AI feasible."}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"gem-gpu-accelerated-module-structure",children:"GEM (GPU-accelerated Module) Structure"}),"\n",(0,s.jsx)(n.p,{children:"Each Isaac ROS GEM follows a standard pipeline:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input"}),": ROS 2 messages (images, point clouds, IMU data)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Processing"}),": CUDA kernels for parallel computation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": ROS 2 messages (poses, detections, maps)"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Conceptual GEM pipeline\ndef isaac_ros_gem(input_msg):\n    # Transfer data to GPU memory\n    gpu_data = cuda.to_device(input_msg.data)\n\n    # Parallel processing on thousands of CUDA cores\n    result = cuda_kernel_process(gpu_data)\n\n    # Transfer back to CPU for ROS publishing\n    output_msg = cuda.from_device(result)\n    return output_msg\n"})}),"\n",(0,s.jsx)(n.h2,{id:"installation-on-jetson-orin",children:"Installation on Jetson Orin"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Jetson Orin is the recommended platform for deploying Isaac ROS on physical robots."}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware"}),": Jetson Orin Nano/NX/AGX (8GB+ RAM recommended)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"JetPack"}),": 5.1 or later (includes CUDA, TensorRT, cuDNN)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2"}),": Humble Hawksbill"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation-steps",children:"Installation Steps"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS base dependencies\nsudo apt-get update\nsudo apt-get install -y \\\n    python3-pip \\\n    libopencv-dev \\\n    python3-opencv\n\n# Create workspace for Isaac ROS\nmkdir -p ~/isaac_ros_ws/src\ncd ~/isaac_ros_ws/src\n\n# Clone Isaac ROS common (required by all GEMs)\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\n\n# Clone specific GEMs (we'll use Visual SLAM)\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\n\n# Install dependencies using rosdep\ncd ~/isaac_ros_ws\nrosdep install --from-paths src --ignore-src -r -y\n\n# Build with colcon\ncolcon build --symlink-install\n\n# Source workspace\nsource ~/isaac_ros_ws/install/setup.bash\n"})}),"\n",(0,s.jsx)(n.h2,{id:"hardware-accelerated-visual-slam",children:"Hardware-Accelerated Visual SLAM"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) enables robots to build maps while tracking their position using only camera input."}),"\n",(0,s.jsx)(n.h3,{id:"traditional-cpu-vslam",children:"Traditional CPU VSLAM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ORB-SLAM3"}),": ~5-15 Hz on CPU, high latency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RTAB-Map"}),": ~10-20 Hz, struggles with high-resolution input"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-vslam",children:"Isaac ROS VSLAM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance"}),": 30-60 Hz on Jetson Orin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency"}),": < 33ms (suitable for real-time control)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality"}),": Leverages GPU for dense feature extraction"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"running-isaac-ros-vslam",children:"Running Isaac ROS VSLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Python launch file for Isaac ROS Visual SLAM\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Camera driver node (replace with your camera)\n        Node(\n            package='realsense2_camera',\n            executable='realsense2_camera_node',\n            name='camera',\n            parameters=[{\n                'enable_depth': True,\n                'depth_module.profile': '640x480x30',  # Resolution @ FPS\n                'rgb_camera.profile': '640x480x30'\n            }]\n        ),\n\n        # Isaac ROS Visual SLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='isaac_ros_visual_slam',\n            name='visual_slam',\n            parameters=[{\n                'enable_imu': False,          # Set True if IMU available\n                'enable_rectified_pose': True, # Correct for camera distortion\n                'denoise_input_images': True,  # GPU-accelerated denoising\n                'rectified_images': True,\n                'enable_debug_mode': False,\n                'debug_dump_path': '/tmp/vslam_debug',\n                # Feature detection parameters\n                'num_cameras': 1,              # Single camera SLAM\n                'min_num_images': 10,          # Keyframes before map init\n                'horizontal_stereo_camera': False\n            }],\n            remappings=[\n                # Map camera topics to VSLAM inputs\n                ('stereo_camera/left/image', '/camera/color/image_raw'),\n                ('stereo_camera/left/camera_info', '/camera/color/camera_info'),\n                # VSLAM outputs\n                ('visual_slam/tracking/odometry', '/odom'),\n                ('visual_slam/tracking/vo_pose', '/vo_pose')\n            ]\n        ),\n\n        # Visualization in RViz\n        Node(\n            package='rviz2',\n            executable='rviz2',\n            name='rviz2',\n            arguments=['-d', './config/vslam.rviz']\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"stereo-vision-for-depth-perception",children:"Stereo Vision for Depth Perception"}),"\n",(0,s.jsx)(n.p,{children:"Stereo cameras provide dense depth maps for obstacle avoidance and manipulation."}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-stereo-disparity",children:"Isaac ROS Stereo Disparity"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Stereo camera driver\n        Node(\n            package='realsense2_camera',\n            executable='realsense2_camera_node',\n            parameters=[{\n                'enable_depth': False,  # Use stereo instead of active depth\n                'enable_infra1': True,  # Left IR camera\n                'enable_infra2': True,  # Right IR camera\n                'infra_width': 1280,\n                'infra_height': 720,\n                'infra_fps': 30\n            }]\n        ),\n\n        # Isaac ROS Stereo Disparity (GPU-accelerated)\n        Node(\n            package='isaac_ros_stereo_image_proc',\n            executable='disparity_node',\n            parameters=[{\n                'backends': 'CUDA',     # Force GPU execution\n                'max_disparity': 128.0, # Max depth range\n                'window_size': 5        # Matching window (odd number)\n            }],\n            remappings=[\n                ('left/image_rect', '/camera/infra1/image_rect_raw'),\n                ('left/camera_info', '/camera/infra1/camera_info'),\n                ('right/image_rect', '/camera/infra2/image_rect_raw'),\n                ('right/camera_info', '/camera/infra2/camera_info'),\n                ('disparity', '/disparity')\n            ]\n        ),\n\n        # Convert disparity to point cloud\n        Node(\n            package='isaac_ros_stereo_image_proc',\n            executable='point_cloud_node',\n            parameters=[{\n                'use_color': False  # Set True if RGB available\n            }],\n            remappings=[\n                ('disparity', '/disparity'),\n                ('left/camera_info', '/camera/infra1/camera_info'),\n                ('points2', '/points')\n            ]\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"dnn-inference-with-tensorrt",children:"DNN Inference with TensorRT"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS uses TensorRT to optimize neural networks for NVIDIA hardware."}),"\n",(0,s.jsx)(n.h3,{id:"converting-pytorch-model-to-tensorrt",children:"Converting PyTorch Model to TensorRT"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.onnx\nfrom torch2trt import TRTModule\n\n# 1. Export PyTorch model to ONNX\nmodel = torch.load('object_detector.pth')\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 640, 640).cuda()  # Batch size 1, 3 channels, 640x640\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"object_detector.onnx\",\n    input_names=['input'],\n    output_names=['boxes', 'scores', 'classes'],\n    dynamic_axes={\n        'input': {0: 'batch_size'},  # Variable batch size\n        'boxes': {0: 'batch_size'},\n        'scores': {0: 'batch_size'}\n    }\n)\n\n# 2. Convert ONNX to TensorRT engine\nimport tensorrt as trt\n\nlogger = trt.Logger(trt.Logger.WARNING)\nbuilder = trt.Builder(logger)\nnetwork = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nparser = trt.OnnxParser(network, logger)\n\n# Parse ONNX model\nwith open('object_detector.onnx', 'rb') as model_file:\n    if not parser.parse(model_file.read()):\n        for error in range(parser.num_errors):\n            print(parser.get_error(error))\n\n# Configure optimization (FP16 for Jetson)\nconfig = builder.create_builder_config()\nconfig.set_flag(trt.BuilderFlag.FP16)  # Half-precision for 2x speedup\nconfig.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\n\n# Build optimized engine\nserialized_engine = builder.build_serialized_network(network, config)\nwith open('object_detector.trt', 'wb') as f:\n    f.write(serialized_engine)\n\nprint(\"TensorRT engine saved - optimized for Jetson Orin\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Algorithm"}),(0,s.jsx)(n.th,{children:"CPU (i7-12700)"}),(0,s.jsx)(n.th,{children:"GPU (RTX 3060)"}),(0,s.jsx)(n.th,{children:"Jetson Orin"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"VSLAM"}),(0,s.jsx)(n.td,{children:"12 Hz"}),(0,s.jsx)(n.td,{children:"90 Hz"}),(0,s.jsx)(n.td,{children:"45 Hz"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Stereo"}),(0,s.jsx)(n.td,{children:"8 Hz"}),(0,s.jsx)(n.td,{children:"120 Hz"}),(0,s.jsx)(n.td,{children:"60 Hz"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"YOLOv8"}),(0,s.jsx)(n.td,{children:"15 Hz"}),(0,s.jsx)(n.td,{children:"180 Hz"}),(0,s.jsx)(n.td,{children:"80 Hz"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VSLAM Deployment"}),": Run Isaac ROS Visual SLAM with a webcam and observe pose estimates in RViz."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stereo Depth"}),": If you have a stereo camera, generate a point cloud and visualize it. Otherwise, use recorded ROS bags from Isaac ROS examples."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"TensorRT Conversion"}),": Convert a simple PyTorch classifier to TensorRT and measure inference time before/after."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GEM Integration"}),": Create a launch file that runs VSLAM + stereo disparity + object detection simultaneously."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/hackathon-book/module-03-isaac/week-9-perception",children:"Week 9 - Perception for Manipulation"})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);