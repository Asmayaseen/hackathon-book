"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[3738],{5368:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-04-vla/week-11-llm-planning","title":"Week 11: LLM-Based Task Planning","description":"Using GPT-4 for Robot Task Planning","source":"@site/docs/module-04-vla/week-11-llm-planning.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-11-llm-planning","permalink":"/hackathon-book/module-04-vla/week-11-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-04-vla/week-11-llm-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 11: Voice-to-Action Pipeline","permalink":"/hackathon-book/module-04-vla/week-11-voice-to-action"},"next":{"title":"Week 12: Multimodal Vision-Language Fusion","permalink":"/hackathon-book/module-04-vla/week-12-multimodal"}}');var i=t(4848),a=t(8453);const s={},r="Week 11: LLM-Based Task Planning",l={},c=[{value:"Using GPT-4 for Robot Task Planning",id:"using-gpt-4-for-robot-task-planning",level:2},{value:"Why LLMs for Planning?",id:"why-llms-for-planning",level:3},{value:"Basic Task Decomposition",id:"basic-task-decomposition",level:3},{value:"The ReAct Pattern: Reasoning + Acting",id:"the-react-pattern-reasoning--acting",level:2},{value:"ReAct Architecture",id:"react-architecture",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Chain-of-Thought for Robots",id:"chain-of-thought-for-robots",level:2},{value:"CoT for Manipulation Planning",id:"cot-for-manipulation-planning",level:3},{value:"Prompt Engineering Best Practices",id:"prompt-engineering-best-practices",level:2},{value:"Practice Exercise",id:"practice-exercise",level:3},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"week-11-llm-based-task-planning",children:"Week 11: LLM-Based Task Planning"})}),"\n",(0,i.jsx)(e.h2,{id:"using-gpt-4-for-robot-task-planning",children:"Using GPT-4 for Robot Task Planning"}),"\n",(0,i.jsx)(e.p,{children:'Large Language Models excel at decomposing high-level goals into executable sub-tasks by leveraging common-sense reasoning learned from internet-scale text data. For robotics, this means translating vague instructions like "prepare breakfast" into concrete action sequences: navigate to kitchen, open fridge, retrieve eggs, close fridge, navigate to stove, etc.'}),"\n",(0,i.jsx)(e.h3,{id:"why-llms-for-planning",children:"Why LLMs for Planning?"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Generalization"}),": Traditional planners (PDDL, hierarchical task networks) require manually authored domain models. LLMs learn task structure implicitly from diverse text\u2014recipes, instruction manuals, how-to guides\u2014enabling zero-shot planning for novel tasks."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Contextual Reasoning"}),': LLMs infer implicit constraints. Given "make coffee," they know to check if water is available before attempting to brew, without explicit preconditions in code.']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Interface"}),': Users provide goals in free-form language instead of formal specifications. "Help me set the table for dinner" is more intuitive than ',(0,i.jsx)(e.code,{children:"set_table(num_plates=4, silverware=True)"}),"."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Failure Recovery"}),": When plans fail, LLMs replan based on error descriptions, exhibiting adaptive behavior rare in classical planners."]}),"\n",(0,i.jsx)(e.h3,{id:"basic-task-decomposition",children:"Basic Task Decomposition"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\nimport os\nfrom typing import List, Dict\n\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\ndef plan_task(goal: str, context: str = "") -> List[str]:\n    """\n    Generate step-by-step plan for achieving robot goal.\n\n    Args:\n        goal: High-level objective (e.g., "clean the living room")\n        context: Environmental state (e.g., "vacuum is in closet, trash bin is full")\n\n    Returns:\n        List of action steps in execution order\n    """\n    # System prompt defines robot\'s action space and constraints\n    system_prompt = """You are a task planner for a humanoid robot.\n    Break down user goals into sequential steps using available actions.\n\n    Available actions:\n    - navigate(location): Move to specified location\n    - pick(object): Grasp object in front of robot\n    - place(object, location): Put held object at location\n    - open(object): Open door/drawer/container\n    - close(object): Close door/drawer/container\n    - activate(device): Turn on appliance/switch\n    - deactivate(device): Turn off appliance/switch\n\n    Constraints:\n    - Robot can hold one object at a time (must place before picking new object)\n    - Navigation requires clear path (cannot walk through closed doors)\n    - Object must be within reach (navigate first if not visible)\n\n    Return numbered list of steps. Be concise.\n    """\n\n    # Include environmental context to inform plan\n    user_prompt = f"""Goal: {goal}\n\n    Current environment: {context if context else "Unknown state"}\n\n    Generate action plan:\n    """\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": user_prompt}\n        ],\n        temperature=0.3,  # Low temperature for consistent planning\n        max_tokens=300\n    )\n\n    # Parse numbered list into steps\n    plan_text = response.choices[0].message.content\n    steps = [\n        line.strip()\n        for line in plan_text.split(\'\\n\')\n        if line.strip() and line.strip()[0].isdigit()\n    ]\n    return steps\n\n# Example: Home assistance task\ngoal = "Prepare a cup of coffee"\ncontext = "You are in the living room. Coffee maker is in the kitchen."\n\nplan = plan_task(goal, context)\nfor i, step in enumerate(plan, 1):\n    print(f"{i}. {step}")\n\n# Output:\n# 1. navigate(kitchen)\n# 2. open(cabinet) to access coffee grounds\n# 3. pick(coffee_grounds)\n# 4. place(coffee_grounds, coffee_maker)\n# 5. close(cabinet)\n# 6. activate(coffee_maker)\n# 7. pick(mug)\n# 8. place(mug, coffee_maker)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"the-react-pattern-reasoning--acting",children:"The ReAct Pattern: Reasoning + Acting"}),"\n",(0,i.jsxs)(e.p,{children:["The ",(0,i.jsx)(e.strong,{children:"ReAct"})," (Reasoning and Acting) pattern interleaves thought generation with action execution, allowing LLMs to adaptively adjust plans based on real-time feedback. Unlike open-loop planning (generate entire plan upfront), ReAct performs closed-loop reasoning at each step."]}),"\n",(0,i.jsx)(e.h3,{id:"react-architecture",children:"ReAct Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Observation \u2192 Thought \u2192 Action \u2192 Observation \u2192 Thought \u2192 Action \u2192 ...\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Observation"}),": Current world state from sensors (object positions, door status, battery level)\n",(0,i.jsx)(e.strong,{children:"Thought"}),": LLM-generated reasoning about what to do next\n",(0,i.jsx)(e.strong,{children:"Action"}),": Executable robot command\n",(0,i.jsx)(e.strong,{children:"Loop"}),": Repeat until goal achieved or failure detected"]}),"\n",(0,i.jsx)(e.h3,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import json\nfrom typing import Tuple, Optional\n\nclass ReActAgent:\n    """\n    ReAct agent for adaptive robot task execution.\n    Combines LLM reasoning with environmental feedback.\n    """\n\n    def __init__(self, max_steps: int = 10):\n        self.max_steps = max_steps\n        self.history = []  # Track (thought, action, observation) tuples\n\n    def run(self, goal: str, get_observation_fn) -> bool:\n        """\n        Execute goal using ReAct loop.\n\n        Args:\n            goal: Task objective\n            get_observation_fn: Callable returning current world state\n\n        Returns:\n            bool: True if goal achieved, False if failed\n        """\n        observation = get_observation_fn()\n        self.history = []\n\n        for step in range(self.max_steps):\n            # Generate thought and action from LLM\n            thought, action = self._reason(goal, observation)\n            self.history.append((thought, action, observation))\n\n            print(f"\\n--- Step {step + 1} ---")\n            print(f"Observation: {observation}")\n            print(f"Thought: {thought}")\n            print(f"Action: {action}")\n\n            # Check for task completion or failure\n            if action == "DONE":\n                print("Goal achieved!")\n                return True\n            if action == "FAIL":\n                print(f"Task failed: {thought}")\n                return False\n\n            # Execute action and get new observation\n            observation = self._execute_action(action, get_observation_fn)\n\n        print("Max steps reached without completion.")\n        return False\n\n    def _reason(self, goal: str, observation: str) -> Tuple[str, str]:\n        """\n        Generate reasoning and next action.\n\n        Returns:\n            (thought, action): Reasoning string and action command\n        """\n        # Build prompt with full history for context\n        history_text = "\\n".join([\n            f"Thought: {t}\\nAction: {a}\\nObservation: {o}"\n            for t, a, o in self.history\n        ])\n\n        prompt = f"""You are controlling a humanoid robot. Use step-by-step reasoning.\n\nGoal: {goal}\n\nPrevious steps:\n{history_text if history_text else "None (this is the first step)"}\n\nCurrent observation: {observation}\n\nThink about what to do next. Format your response as:\nThought: [your reasoning]\nAction: [command]\n\nUse \'DONE\' action when goal is achieved, \'FAIL\' if impossible.\nAvailable actions: navigate(loc), pick(obj), place(obj, loc), open(obj), close(obj), check(obj).\n"""\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.2,\n            max_tokens=150\n        )\n\n        # Parse thought and action from response\n        text = response.choices[0].message.content\n        lines = text.strip().split(\'\\n\')\n\n        thought = ""\n        action = ""\n        for line in lines:\n            if line.startswith("Thought:"):\n                thought = line.replace("Thought:", "").strip()\n            elif line.startswith("Action:"):\n                action = line.replace("Action:", "").strip()\n\n        return thought, action\n\n    def _execute_action(self, action: str, get_observation_fn) -> str:\n        """\n        Execute action command (mock implementation).\n        In real system, this calls ROS 2 action servers.\n        """\n        # Simulate action execution (replace with actual robot interface)\n        print(f"Executing: {action}")\n\n        # Return updated observation after action\n        # In production: query sensors/perception system\n        return get_observation_fn()\n\n# Example usage with mock environment\ndef mock_get_observation():\n    """\n    Mock sensor reading (replace with actual perception system).\n    In real system: query object detector, SLAM map, joint states.\n    """\n    # Simulate changing observations based on agent\'s actions\n    observations = [\n        "You are in living room. Coffee maker visible in kitchen.",\n        "You are in kitchen. Coffee maker is on counter. Mug is in cabinet.",\n        "Cabinet is open. Mug is visible.",\n        "You are holding mug. Coffee maker has finished brewing.",\n        "Mug is placed under coffee maker spout."\n    ]\n    # Cycle through observations (simplified; real system queries sensors)\n    return observations[len(agent.history) % len(observations)]\n\n# Run ReAct agent\nagent = ReActAgent(max_steps=8)\nsuccess = agent.run(\n    goal="Serve a cup of coffee",\n    get_observation_fn=mock_get_observation\n)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"chain-of-thought-for-robots",children:"Chain-of-Thought for Robots"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Chain-of-Thought (CoT)"})," prompting elicits intermediate reasoning steps, improving LLM performance on complex tasks. For robotics, CoT helps decompose spatiotemporal reasoning:"]}),"\n",(0,i.jsx)(e.h3,{id:"cot-for-manipulation-planning",children:"CoT for Manipulation Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def plan_grasp_with_cot(object_name: str, scene_description: str) -> Dict:\n    """\n    Generate grasp plan using chain-of-thought reasoning.\n\n    Args:\n        object_name: Target object to grasp\n        scene_description: Visual scene layout\n\n    Returns:\n        dict: Grasp strategy with approach vector, contact points\n    """\n    prompt = f"""You are planning a grasp for a humanoid robot arm.\n\nObject: {object_name}\nScene: {scene_description}\n\nThink step-by-step:\n1. What is the object\'s shape and likely material?\n2. What grasp type is appropriate (pinch, power, lateral)?\n3. From which direction should the robot approach?\n4. What are potential failure modes (slipping, collision)?\n\nThen output JSON grasp plan:\n{{\n    "grasp_type": "pinch | power | lateral",\n    "approach_direction": "top | side | front",\n    "contact_points": ["finger1_location", "finger2_location"],\n    "expected_force": "low | medium | high"\n}}\n"""\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": prompt}],\n        temperature=0.1\n    )\n\n    # Extract JSON from response\n    text = response.choices[0].message.content\n    # Find JSON block in response\n    start = text.find(\'{\')\n    end = text.rfind(\'}\') + 1\n    if start != -1 and end > start:\n        grasp_plan = json.loads(text[start:end])\n        return grasp_plan\n\n    return {"error": "Failed to generate grasp plan"}\n\n# Example: Grasp planning\nresult = plan_grasp_with_cot(\n    object_name="wine glass",\n    scene_description="Wine glass on table, stem visible, surrounded by plates"\n)\nprint(json.dumps(result, indent=2))\n'})}),"\n",(0,i.jsx)(e.h2,{id:"prompt-engineering-best-practices",children:"Prompt Engineering Best Practices"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"System Role"}),": Define robot's capabilities and constraints clearly\n",(0,i.jsx)(e.strong,{children:"Few-Shot Examples"}),": Provide 2-3 example plans for similar tasks\n",(0,i.jsx)(e.strong,{children:"Output Format"}),": Specify structured output (JSON, numbered lists) for reliable parsing\n",(0,i.jsx)(e.strong,{children:"Context Window"}),": Include recent observations (last 5 steps) but prune old history to stay within token limits\n",(0,i.jsx)(e.strong,{children:"Error Handling"}),': Prompt LLM to output "UNCERTAIN" when insufficient information; query human for clarification']}),"\n",(0,i.jsx)(e.h3,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,i.jsx)(e.p,{children:"Implement a ReAct agent that:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:'Takes high-level goal: "Set the dinner table for 4 people"'}),"\n",(0,i.jsx)(e.li,{children:"Queries mock environment for object locations"}),"\n",(0,i.jsx)(e.li,{children:"Generates plans adaptively (if plates unavailable, try alternative location)"}),"\n",(0,i.jsx)(e.li,{children:"Handles failures gracefully (if object too heavy, request human assistance)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["Extend with ",(0,i.jsx)(e.strong,{children:"reflection"}),": after failed attempt, prompt LLM to analyze failure and propose alternative strategy."]}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(e.p,{children:["You've implemented LLM-based planning for sequential task execution. In ",(0,i.jsx)(e.strong,{children:"Week 12: Multimodal Integration"}),', you\'ll connect language understanding to visual perception using CLIP, enabling robots to ground commands like "the red object" to specific scene entities.']})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);