"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[7324],{415:(n,e,t)=>{t.d(e,{A:()=>i});const i=t.p+"assets/images/ai-12-9b64b408e75756347a6980d036ec6d0f.png"},5833:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-04-vla/week-11-voice-to-action","title":"Week 11: Voice-to-Action Pipeline","description":"Voice-to-Action Pipeline","source":"@site/docs/module-04-vla/week-11-voice-to-action.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-11-voice-to-action","permalink":"/hackathon-book/module-04-vla/week-11-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-04-vla/week-11-voice-to-action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action Integration","permalink":"/hackathon-book/module-04-vla/intro"},"next":{"title":"Week 11: LLM-Based Task Planning","permalink":"/hackathon-book/module-04-vla/week-11-llm-planning"}}');var o=t(4848),r=t(8453);const a={},s="Week 11: Voice-to-Action Pipeline",c={},l=[{value:"Speech Recognition with OpenAI Whisper",id:"speech-recognition-with-openai-whisper",level:2},{value:"Why Whisper for Robotics?",id:"why-whisper-for-robotics",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Command Parsing and Intent Classification",id:"command-parsing-and-intent-classification",level:2},{value:"Rule-Based Parsing",id:"rule-based-parsing",level:3},{value:"LLM-Based Intent Classification",id:"llm-based-intent-classification",level:3},{value:"Voice Interface Design",id:"voice-interface-design",level:2},{value:"Practice Exercise",id:"practice-exercise",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"week-11-voice-to-action-pipeline",children:"Week 11: Voice-to-Action Pipeline"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Voice-to-Action Pipeline",src:t(415).A+"",width:"736",height:"1104"})}),"\n",(0,o.jsx)(e.h2,{id:"speech-recognition-with-openai-whisper",children:"Speech Recognition with OpenAI Whisper"}),"\n",(0,o.jsx)(e.p,{children:"OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) system trained on 680,000 hours of multilingual data. Unlike traditional ASR models requiring custom wake-word detection and language-specific training, Whisper provides zero-shot transcription across 99 languages with robust performance in noisy environments\u2014critical for humanoid robots operating in homes, factories, and public spaces."}),"\n",(0,o.jsx)(e.h3,{id:"why-whisper-for-robotics",children:"Why Whisper for Robotics?"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Noise Robustness"}),": Whisper's training included diverse acoustic conditions (background music, overlapping speech, machinery noise). A humanoid working in a kitchen can transcribe commands over running dishwashers and conversations."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Multilingual Support"}),": Global deployments require multi-language interfaces. Whisper handles code-switching (mixing languages mid-sentence) common in multilingual households."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"No Fine-Tuning Required"}),': Unlike domain-specific ASR models, Whisper generalizes to robotics vocabulary ("grasp the Phillips screwdriver") without custom training.']}),"\n",(0,o.jsx)(e.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Install Whisper and dependencies\n# Requires ffmpeg for audio processing: sudo apt install ffmpeg\nimport subprocess\nsubprocess.run(["pip", "install", "openai-whisper", "sounddevice", "numpy"])\n\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nfrom scipy.io.wavfile import write\n\n# Load Whisper model (options: tiny, base, small, medium, large)\n# Trade-off: larger models = better accuracy but slower inference\n# For real-time robotics: \'base\' (74M params) achieves <0.5s latency on GPU\nmodel = whisper.load_model("base")  # Downloads ~140MB on first run\n\ndef record_audio(duration=5, sample_rate=16000):\n    """\n    Record audio from microphone for specified duration.\n\n    Args:\n        duration: Recording length in seconds\n        sample_rate: Hz (16kHz is Whisper\'s native rate, avoids resampling)\n\n    Returns:\n        numpy array: Audio samples in range [-1, 1]\n    """\n    print(f"Recording for {duration} seconds...")\n    # Record from default microphone (set device index for specific mic)\n    audio = sd.rec(\n        int(duration * sample_rate),\n        samplerate=sample_rate,\n        channels=1,  # Mono audio\n        dtype=\'float32\'\n    )\n    sd.wait()  # Block until recording completes\n    print("Recording complete.")\n    return audio.flatten()\n\ndef transcribe_audio(audio_array):\n    """\n    Transcribe audio to text using Whisper.\n\n    Args:\n        audio_array: NumPy array of audio samples\n\n    Returns:\n        dict: Transcription result with \'text\', \'language\', \'segments\'\n    """\n    # Whisper expects float32 audio normalized to [-1, 1]\n    result = model.transcribe(\n        audio_array,\n        language=\'en\',  # Set to None for auto-detection (adds latency)\n        task=\'transcribe\',  # Alternative: \'translate\' for non-English to English\n        fp16=True  # Enable half-precision for 2x speed on GPU\n    )\n    return result\n\n# Example usage: Record and transcribe\naudio = record_audio(duration=5)\nresult = transcribe_audio(audio)\nprint(f"Transcription: {result[\'text\']}")\n# Output example: "Robot, pick up the red mug and place it on the table."\n'})}),"\n",(0,o.jsx)(e.h2,{id:"command-parsing-and-intent-classification",children:"Command Parsing and Intent Classification"}),"\n",(0,o.jsxs)(e.p,{children:["Raw transcriptions require parsing into structured robot commands. We extract ",(0,o.jsx)(e.strong,{children:"intent"})," (what action), ",(0,o.jsx)(e.strong,{children:"entities"})," (target objects), and ",(0,o.jsx)(e.strong,{children:"parameters"})," (locations, quantities)."]}),"\n",(0,o.jsx)(e.h3,{id:"rule-based-parsing",children:"Rule-Based Parsing"}),"\n",(0,o.jsx)(e.p,{children:"For constrained vocabularies (warehouse robots with fixed commands), rule-based parsing suffices:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Optional\n\nclass CommandParser:\n    \"\"\"\n    Parse natural language into structured robot commands.\n    Handles imperative sentences with action verbs and object references.\n    \"\"\"\n\n    # Define action vocabulary with synonyms\n    ACTION_VERBS = {\n        'pick': ['pick', 'grab', 'grasp', 'take', 'lift'],\n        'place': ['place', 'put', 'set', 'drop', 'position'],\n        'navigate': ['go', 'move', 'walk', 'navigate', 'travel'],\n        'open': ['open'],\n        'close': ['close', 'shut']\n    }\n\n    # Spatial prepositions for location extraction\n    LOCATIONS = ['on', 'in', 'under', 'next to', 'above', 'below', 'near']\n\n    def __init__(self):\n        # Compile regex patterns for efficiency\n        self.action_pattern = self._build_action_pattern()\n\n    def _build_action_pattern(self) -> re.Pattern:\n        \"\"\"Create regex matching any action verb.\"\"\"\n        all_verbs = [v for synonyms in self.ACTION_VERBS.values() for v in synonyms]\n        pattern = r'\\b(' + '|'.join(all_verbs) + r')\\b'\n        return re.compile(pattern, re.IGNORECASE)\n\n    def parse(self, text: str) -> Dict:\n        \"\"\"\n        Extract intent and entities from command.\n\n        Args:\n            text: Natural language command\n\n        Returns:\n            dict: {\n                'intent': str,\n                'object': str,\n                'location': str,\n                'confidence': float\n            }\n        \"\"\"\n        text = text.lower().strip()\n\n        # Extract action intent\n        action_match = self.action_pattern.search(text)\n        if not action_match:\n            return {'intent': 'unknown', 'confidence': 0.0}\n\n        verb = action_match.group(1)\n        intent = self._map_verb_to_intent(verb)\n\n        # Extract target object (noun after action verb)\n        object_match = re.search(\n            r'\\b(?:the\\s+)?(\\w+(?:\\s+\\w+)?)\\b',  # Captures \"red mug\" or \"mug\"\n            text[action_match.end():]\n        )\n        target_object = object_match.group(1) if object_match else None\n\n        # Extract location (prepositional phrase)\n        location = self._extract_location(text)\n\n        return {\n            'intent': intent,\n            'object': target_object,\n            'location': location,\n            'confidence': 0.95 if target_object else 0.6\n        }\n\n    def _map_verb_to_intent(self, verb: str) -> str:\n        \"\"\"Map detected verb to canonical intent.\"\"\"\n        for intent, synonyms in self.ACTION_VERBS.items():\n            if verb in synonyms:\n                return intent\n        return 'unknown'\n\n    def _extract_location(self, text: str) -> Optional[str]:\n        \"\"\"Extract location phrase (e.g., 'on the table').\"\"\"\n        for prep in self.LOCATIONS:\n            pattern = f'{prep}\\\\s+(?:the\\\\s+)?(\\\\w+(?:\\\\s+\\\\w+)?)'\n            match = re.search(pattern, text)\n            if match:\n                return f\"{prep} {match.group(1)}\"\n        return None\n\n# Example usage\nparser = CommandParser()\ncommands = [\n    \"Pick up the red mug\",\n    \"Place it on the table\",\n    \"Go to the kitchen\",\n    \"Open the drawer\"\n]\n\nfor cmd in commands:\n    result = parser.parse(cmd)\n    print(f\"Input: {cmd}\")\n    print(f\"Parsed: {result}\\n\")\n\n# Output:\n# Input: Pick up the red mug\n# Parsed: {'intent': 'pick', 'object': 'red mug', 'location': None, 'confidence': 0.95}\n"})}),"\n",(0,o.jsx)(e.h3,{id:"llm-based-intent-classification",children:"LLM-Based Intent Classification"}),"\n",(0,o.jsx)(e.p,{children:"For open-ended commands, use GPT-4 for semantic understanding:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\nimport os\nimport json\n\n# Set API key from environment variable (never hardcode secrets)\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\ndef classify_intent_with_llm(command: str) -> Dict:\n    """\n    Use GPT-4 to extract structured intent from natural language.\n    Handles complex commands like \'After closing the door, bring me water.\'\n\n    Args:\n        command: Natural language instruction\n\n    Returns:\n        dict: Structured command with intent, parameters, and sequence\n    """\n    # System prompt defines robot\'s capabilities and output format\n    system_prompt = """You are a command parser for a humanoid robot.\n    Extract intent, objects, locations, and action sequences from user commands.\n\n    Available actions: pick, place, navigate, open, close, wait.\n\n    Return JSON with format:\n    {\n        "actions": [\n            {"intent": "action", "object": "item", "location": "place", "parameters": {}}\n        ],\n        "confidence": 0.0-1.0\n    }\n    """\n\n    # Few-shot examples improve parsing accuracy\n    user_prompt = f"""Command: {command}\n\n    Examples:\n    Input: "Grab the blue cup and put it in the sink"\n    Output: {{"actions": [{{"intent": "pick", "object": "blue cup"}}, {{"intent": "place", "location": "sink"}}], "confidence": 0.95}}\n\n    Input: "Go to the bedroom"\n    Output: {{"actions": [{{"intent": "navigate", "location": "bedroom"}}], "confidence": 0.98}}\n\n    Now parse the command above.\n    """\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",  # Use gpt-3.5-turbo for faster/cheaper inference\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": user_prompt}\n        ],\n        temperature=0.0,  # Deterministic output for command parsing\n        max_tokens=200\n    )\n\n    # Extract JSON from response\n    result_text = response.choices[0].message.content\n    try:\n        return json.loads(result_text)\n    except json.JSONDecodeError:\n        return {"actions": [], "confidence": 0.0, "error": "Parse failed"}\n\n# Example: Complex multi-step command\ncommand = "First go to the kitchen, then pick up the green bottle and bring it here."\nintent = classify_intent_with_llm(command)\nprint(json.dumps(intent, indent=2))\n\n# Output:\n# {\n#   "actions": [\n#     {"intent": "navigate", "location": "kitchen"},\n#     {"intent": "pick", "object": "green bottle"},\n#     {"intent": "navigate", "location": "user"}\n#   ],\n#   "confidence": 0.92\n# }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"voice-interface-design",children:"Voice Interface Design"}),"\n",(0,o.jsx)(e.p,{children:"Effective human-robot voice interaction requires:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Wake Word Detection"}),': Use lightweight models (Porcupine, Snowboy) to activate listening only on "Hey Robot", reducing false activations.']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Confirmation Loops"}),': Repeat parsed commands back to user: "I will pick up the red mug. Proceed?" Prevents misunderstandings before execution.']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Feedback"}),': Provide audio acknowledgments during long operations: "Navigating to kitchen... Arrived. Searching for green bottle..."']}),"\n",(0,o.jsx)(e.h3,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,o.jsx)(e.p,{children:"Implement a complete voice command pipeline:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Record 5-second audio clip"}),"\n",(0,o.jsx)(e.li,{children:"Transcribe with Whisper"}),"\n",(0,o.jsx)(e.li,{children:"Parse intent with rule-based parser"}),"\n",(0,o.jsx)(e.li,{children:"Generate ROS 2 action goal for robot execution"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:'Extend the parser to handle negations ("Don\'t pick up the blue cup") and conditionals ("If the door is open, go through it").'}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(e.p,{children:["You've built the perception layer (speech \u2192 text \u2192 intent). In ",(0,o.jsx)(e.strong,{children:"Week 11: LLM Planning"}),", you'll use GPT-4 to generate multi-step task plans, implementing the ReAct pattern for adaptive reasoning during execution."]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);