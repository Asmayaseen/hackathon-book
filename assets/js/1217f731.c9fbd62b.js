"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[6031],{7401:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-02-simulation/week-7-sensors","title":"Week 7: Sensor Simulation","description":"Sensor Simulation","source":"@site/docs/module-02-simulation/week-7-sensors.md","sourceDirName":"module-02-simulation","slug":"/module-02-simulation/week-7-sensors","permalink":"/hackathon-book/module-02-simulation/week-7-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-02-simulation/week-7-sensors.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 6: Physics Configuration for Humanoid Robots","permalink":"/hackathon-book/module-02-simulation/week-6-physics"},"next":{"title":"Week 7: Unity for Robotics","permalink":"/hackathon-book/module-02-simulation/week-7-unity"}}');var i=s(4848),t=s(8453);const r={},o="Week 7: Sensor Simulation",l={},c=[{value:"Why Sensor Simulation Matters",id:"why-sensor-simulation-matters",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Depth Cameras (Intel RealSense D435)",id:"depth-cameras-intel-realsense-d435",level:2},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:2},{value:"Joint Encoders and Noise Modeling",id:"joint-encoders-and-noise-modeling",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-7-sensor-simulation",children:"Week 7: Sensor Simulation"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sensor Simulation",src:s(9176).A+"",width:"736",height:"1288"})}),"\n",(0,i.jsx)(n.h2,{id:"why-sensor-simulation-matters",children:"Why Sensor Simulation Matters"}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems rely on noisy, imperfect sensor data. Simulating sensors with realistic characteristics is critical for sim-to-real transfer. Key challenges:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise and Drift"}),": Real sensors have Gaussian noise, bias drift, and quantization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update Rates"}),": Sensors run at different frequencies (IMU: 200Hz, LiDAR: 10-20Hz, camera: 30Hz)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": Data arrives with delays (10-50ms typical)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environmental Effects"}),": Lighting changes affect cameras, reflective surfaces confuse LiDAR"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"})," measures distances by timing laser pulses. Common in outdoor navigation."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add LiDAR sensor to humanoid head --\x3e\n<link name="head">\n  <pose>0 0 1.7 0 0 0</pose> \x3c!-- 1.7m height --\x3e\n\n  <sensor name="lidar_sensor" type="gpu_lidar">\n    <pose>0 0 0.1 0 0 0</pose> \x3c!-- Offset from head center --\x3e\n    <update_rate>10</update_rate> \x3c!-- 10 Hz (common for robotics) --\x3e\n    <topic>/humanoid/lidar</topic>\n\n    <lidar>\n      <scan>\n        <horizontal>\n          <samples>720</samples> \x3c!-- 720 points per scan --\x3e\n          <resolution>1.0</resolution>\n          <min_angle>-3.14159</min_angle> \x3c!-- -180 degrees --\x3e\n          <max_angle>3.14159</max_angle>  \x3c!-- +180 degrees --\x3e\n        </horizontal>\n        <vertical>\n          <samples>16</samples> \x3c!-- 16 vertical layers --\x3e\n          <resolution>1.0</resolution>\n          <min_angle>-0.2618</min_angle> \x3c!-- -15 degrees --\x3e\n          <max_angle>0.2618</max_angle>  \x3c!-- +15 degrees --\x3e\n        </vertical>\n      </scan>\n\n      <range>\n        <min>0.1</min> \x3c!-- Minimum range: 10cm --\x3e\n        <max>30.0</max> \x3c!-- Maximum range: 30m --\x3e\n        <resolution>0.01</resolution> \x3c!-- 1cm precision --\x3e\n      </range>\n\n      \x3c!-- Realistic noise model --\x3e\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.02</stddev> \x3c!-- 2cm standard deviation --\x3e\n      </noise>\n    </lidar>\n\n    <visualize>true</visualize> \x3c!-- Show rays in GUI --\x3e\n  </sensor>\n</link>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS2 Subscriber (Python)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n        self.subscription = self.create_subscription(\n            LaserScan,\n            '/humanoid/lidar',\n            self.lidar_callback,\n            10)\n\n    def lidar_callback(self, msg):\n        # Extract range data\n        ranges = msg.ranges  # List of distances (meters)\n        angles = [msg.angle_min + i * msg.angle_increment\n                  for i in range(len(ranges))]\n\n        # Detect closest obstacle\n        valid_ranges = [r for r in ranges if msg.range_min < r < msg.range_max]\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            self.get_logger().info(f'Closest obstacle: {min_distance:.2f}m')\n\ndef main():\n    rclpy.init()\n    node = LidarProcessor()\n    rclpy.spin(node)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"depth-cameras-intel-realsense-d435",children:"Depth Cameras (Intel RealSense D435)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Depth cameras"})," provide RGB images with per-pixel depth. Essential for manipulation and obstacle avoidance."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- RealSense D435 mounted on robot chest --\x3e\n<sensor name="depth_camera" type="depth_camera">\n  <pose>0.15 0 1.2 0 0.3 0</pose> \x3c!-- 0.3 rad (17\xb0) downward tilt --\x3e\n  <update_rate>30</update_rate> \x3c!-- 30 FPS --\x3e\n  <topic>/humanoid/depth</topic>\n\n  <camera>\n    <horizontal_fov>1.5184</horizontal_fov> \x3c!-- 87\xb0 (RealSense spec) --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format> \x3c!-- RGB8 --\x3e\n    </image>\n    <clip>\n      <near>0.3</near> \x3c!-- Minimum depth: 30cm --\x3e\n      <far>10.0</far> \x3c!-- Maximum depth: 10m --\x3e\n    </clip>\n\n    \x3c!-- Depth-specific noise (increases with distance) --\x3e\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev> \x3c!-- 7mm at 1m (realistic for D435) --\x3e\n    </noise>\n  </camera>\n\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <argument>image_raw:=rgb/image_raw</argument>\n      <argument>depth/image_raw:=depth/image_raw</argument>\n      <argument>camera_info:=rgb/camera_info</argument>\n    </ros>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing Depth Images (Python + OpenCV)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass DepthProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_processor')\n        self.bridge = CvBridge()\n        self.subscription = self.create_subscription(\n            Image,\n            '/humanoid/depth/image_raw',\n            self.depth_callback,\n            10)\n\n    def depth_callback(self, msg):\n        # Convert ROS Image to OpenCV format (32FC1 - float depth)\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n\n        # Find graspable objects (20cm - 50cm range)\n        mask = cv2.inRange(depth_image, 0.2, 0.5)\n\n        # Morphological operations to clean up noise\n        kernel = np.ones((5,5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find largest contour (potential grasp target)\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n                                       cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest = max(contours, key=cv2.contourArea)\n            M = cv2.moments(largest)\n            if M['m00'] > 0:\n                cx = int(M['m10'] / M['m00'])\n                cy = int(M['m01'] / M['m00'])\n                depth = depth_image[cy, cx]\n                self.get_logger().info(f'Target at ({cx}, {cy}), depth: {depth:.2f}m')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,i.jsxs)(n.p,{children:["IMUs measure ",(0,i.jsx)(n.strong,{children:"linear acceleration"})," and ",(0,i.jsx)(n.strong,{children:"angular velocity"}),". Critical for balance control in humanoids."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU in robot torso (common placement) --\x3e\n<sensor name="imu_sensor" type="imu">\n  <pose>0 0 1.0 0 0 0</pose> \x3c!-- Center of mass --\x3e\n  <update_rate>200</update_rate> \x3c!-- 200 Hz (high-frequency control) --\x3e\n  <topic>/humanoid/imu</topic>\n\n  <imu>\n    \x3c!-- Accelerometer noise --\x3e\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev> \x3c!-- 0.017 m/s\xb2 (typical MEMS IMU) --\x3e\n          <bias_mean>0.05</bias_mean> \x3c!-- 50 mg bias drift --\x3e\n          <bias_stddev>0.01</bias_stddev>\n        </noise>\n      </x>\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.017</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.017</stddev></noise></z>\n    </linear_acceleration>\n\n    \x3c!-- Gyroscope noise --\x3e\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.00087</stddev> \x3c!-- 0.05\xb0/s noise --\x3e\n          <bias_mean>0.0001</bias_mean> \x3c!-- Gyro drift --\x3e\n          <bias_stddev>0.00005</bias_stddev>\n        </noise>\n      </x>\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.00087</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.00087</stddev></noise></z>\n    </angular_velocity>\n  </imu>\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Orientation Estimation with Madgwick Filter"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\nimport numpy as np\n\nclass OrientationEstimator:\n    def __init__(self, beta=0.1):\n        self.quaternion = np.array([1.0, 0.0, 0.0, 0.0])  # w, x, y, z\n        self.beta = beta  # Filter gain\n\n    def update(self, accel, gyro, dt):\n        # Madgwick AHRS algorithm (simplified)\n        # Normalize accelerometer\n        accel = accel / np.linalg.norm(accel)\n\n        # Gradient descent algorithm corrective step\n        q = self.quaternion\n        f = np.array([\n            2*(q[1]*q[3] - q[0]*q[2]) - accel[0],\n            2*(q[0]*q[1] + q[2]*q[3]) - accel[1],\n            2*(0.5 - q[1]**2 - q[2]**2) - accel[2]\n        ])\n\n        # Update quaternion with gyroscope\n        q_dot = 0.5 * self.quaternion_multiply(q, [0, gyro[0], gyro[1], gyro[2]])\n        q = q + q_dot * dt\n\n        # Normalize and store\n        self.quaternion = q / np.linalg.norm(q)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"joint-encoders-and-noise-modeling",children:"Joint Encoders and Noise Modeling"}),"\n",(0,i.jsxs)(n.p,{children:["Real joint encoders have ",(0,i.jsx)(n.strong,{children:"quantization"})," (discrete steps) and ",(0,i.jsx)(n.strong,{children:"backlash"})," (dead zone)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Joint position sensor with realistic noise --\x3e\n<joint name="knee_joint" type="revolute">\n  <sensor name="knee_encoder" type="joint_position">\n    <update_rate>100</update_rate> \x3c!-- 100 Hz --\x3e\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.001</stddev> \x3c!-- 0.001 rad (~0.06\xb0) quantization --\x3e\n    </noise>\n  </sensor>\n</joint>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise"}),": Add all four sensors (LiDAR, depth camera, IMU, joint encoders) to a humanoid model. Create a ROS2 node that fuses IMU and joint encoder data to estimate the robot's tilt angle. Test with a push force applied to the torso."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"/hackathon-book/module-02-simulation/week-7-unity",children:"Week 7 - Unity for Robotics"})," - High-fidelity simulation with Unity Robotics Hub."]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var a=s(6540);const i={},t=a.createContext(i);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:n},e.children)}},9176:(e,n,s)=>{s.d(n,{A:()=>a});const a=s.p+"assets/images/ai-8-243c0894c91b74243651c9ea86ab6cf5.png"}}]);