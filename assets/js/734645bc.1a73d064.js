"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4155],{5998:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-03-isaac/week-8-synthetic-data","title":"Week 8: Synthetic Data Generation","description":"The Synthetic Data Revolution","source":"@site/docs/module-03-isaac/week-8-synthetic-data.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-8-synthetic-data","permalink":"/hackathon-book/module-03-isaac/week-8-synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-03-isaac/week-8-synthetic-data.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 8: Isaac Sim Fundamentals","permalink":"/hackathon-book/module-03-isaac/week-8-isaac-sim"},"next":{"title":"Week 9: Isaac ROS Deployment","permalink":"/hackathon-book/module-03-isaac/week-9-isaac-ros"}}');var a=t(4848),r=t(8453);const s={},o="Week 8: Synthetic Data Generation",c={},l=[{value:"The Synthetic Data Revolution",id:"the-synthetic-data-revolution",level:2},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Randomization Parameters",id:"randomization-parameters",level:3},{value:"Using Isaac Sim Replicator",id:"using-isaac-sim-replicator",level:2},{value:"Basic Synthetic Dataset Generation",id:"basic-synthetic-dataset-generation",level:3},{value:"Perception Ground Truth",id:"perception-ground-truth",level:2},{value:"1. 2D Bounding Boxes",id:"1-2d-bounding-boxes",level:3},{value:"2. Instance Segmentation",id:"2-instance-segmentation",level:3},{value:"3. Depth Maps",id:"3-depth-maps",level:3},{value:"4. 6-DOF Pose",id:"4-6-dof-pose",level:3},{value:"Creating a Custom Dataset for Humanoid Perception",id:"creating-a-custom-dataset-for-humanoid-perception",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-8-synthetic-data-generation",children:"Week 8: Synthetic Data Generation"})}),"\n",(0,a.jsx)(n.h2,{id:"the-synthetic-data-revolution",children:"The Synthetic Data Revolution"}),"\n",(0,a.jsx)(n.p,{children:"Manually labeling training data is expensive and time-consuming. A single annotated image for object detection can cost $0.50-$5.00. For a dataset of 100,000 images, this means $50,000-$500,000 in annotation costs."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Synthetic data generation"})," solves this by automatically creating labeled training data in simulation. Isaac Sim can generate thousands of perfectly labeled images per hour at near-zero marginal cost."]}),"\n",(0,a.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsxs)(n.p,{children:["The key to successful sim-to-real transfer is ",(0,a.jsx)(n.strong,{children:"domain randomization"})," - systematically varying scene parameters to create diversity that covers real-world variations."]}),"\n",(0,a.jsx)(n.h3,{id:"randomization-parameters",children:"Randomization Parameters"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lighting"}),": Intensity, color temperature, angle"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Textures"}),": Surface materials, colors, reflectivity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object poses"}),": Position, rotation, scale"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Camera parameters"}),": FOV, exposure, focus"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Backgrounds"}),": Clutter, distractors, environments"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"using-isaac-sim-replicator",children:"Using Isaac Sim Replicator"}),"\n",(0,a.jsx)(n.p,{children:"Replicator is Isaac Sim's synthetic data generation framework, providing high-level APIs for creating randomized datasets."}),"\n",(0,a.jsx)(n.h3,{id:"basic-synthetic-dataset-generation",children:"Basic Synthetic Dataset Generation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\nfrom omni.isaac.kit import SimulationApp\n\n# Initialize headless simulation for faster generation\nsimulation_app = SimulationApp({"headless": True})\n\n# Define camera for capturing images\ncamera = rep.create.camera(\n    position=(2.0, 2.0, 1.5),  # Camera location\n    look_at=(0, 0, 0)           # Point camera at origin\n)\n\n# Create randomized scene\ndef create_scene():\n    # Random cube representing target object\n    cube = rep.create.cube(\n        position=rep.distribution.uniform((-1, -1, 0.5), (1, 1, 1.5)),\n        scale=rep.distribution.uniform(0.2, 0.5),\n        semantics=[("class", "target_object")]  # Label for detection\n    )\n\n    # Randomize cube material/color\n    with cube:\n        rep.randomizer.color(\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\n        )\n\n    # Random lighting direction and intensity\n    light = rep.create.light(\n        light_type="Distant",  # Directional light (sun-like)\n        intensity=rep.distribution.uniform(500, 3000),\n        rotation=rep.distribution.uniform((0, -180, 0), (0, 180, 0))\n    )\n\n    return cube, light\n\n# Register randomization graph\nrep.randomizer.register(create_scene)\n\n# Configure output writers for different annotation types\n# 1. RGB images\nrgb_writer = rep.WriterRegistry.get("BasicWriter")\nrgb_writer.initialize(\n    output_dir="./synthetic_data/rgb",\n    rgb=True\n)\n\n# 2. Semantic segmentation (pixel-wise class labels)\nsemantic_writer = rep.WriterRegistry.get("BasicWriter")\nsemantic_writer.initialize(\n    output_dir="./synthetic_data/semantic",\n    semantic_segmentation=True\n)\n\n# 3. Bounding boxes for object detection\nbbox_writer = rep.WriterRegistry.get("BasicWriter")\nbbox_writer.initialize(\n    output_dir="./synthetic_data/bbox",\n    bounding_box_2d_tight=True\n)\n\n# Attach all writers to camera\nrgb_writer.attach([camera])\nsemantic_writer.attach([camera])\nbbox_writer.attach([camera])\n\n# Generate 1000 randomized frames\nwith rep.trigger.on_frame(num_frames=1000):\n    rep.randomizer.create_scene()\n\n# Run orchestrator to execute generation\nrep.orchestrator.run()\n\nsimulation_app.close()\nprint("Generated 1000 synthetic training samples")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"perception-ground-truth",children:"Perception Ground Truth"}),"\n",(0,a.jsxs)(n.p,{children:["Isaac Sim automatically generates ",(0,a.jsx)(n.strong,{children:"perfect labels"})," for various perception tasks:"]}),"\n",(0,a.jsx)(n.h3,{id:"1-2d-bounding-boxes",children:"1. 2D Bounding Boxes"}),"\n",(0,a.jsx)(n.p,{children:"Pixel-perfect boxes around objects for object detection training (YOLO, Faster R-CNN)."}),"\n",(0,a.jsx)(n.h3,{id:"2-instance-segmentation",children:"2. Instance Segmentation"}),"\n",(0,a.jsx)(n.p,{children:"Pixel-wise masks identifying individual object instances."}),"\n",(0,a.jsx)(n.h3,{id:"3-depth-maps",children:"3. Depth Maps"}),"\n",(0,a.jsx)(n.p,{children:"Per-pixel distance from camera for stereo vision and 3D reconstruction."}),"\n",(0,a.jsx)(n.h3,{id:"4-6-dof-pose",children:"4. 6-DOF Pose"}),"\n",(0,a.jsx)(n.p,{children:"3D position (x, y, z) and orientation (roll, pitch, yaw) for each object."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Advanced ground truth generation\nimport omni.replicator.core as rep\n\ncamera = rep.create.camera()\n\n# Enable multiple annotation types simultaneously\nwriter = rep.WriterRegistry.get("BasicWriter")\nwriter.initialize(\n    output_dir="./multi_annotation",\n    rgb=True,                          # RGB images\n    bounding_box_2d_tight=True,        # 2D detection boxes\n    bounding_box_3d=True,              # 3D oriented boxes\n    semantic_segmentation=True,        # Pixel-wise class labels\n    instance_segmentation=True,        # Pixel-wise instance IDs\n    distance_to_camera=True,           # Depth map\n    distance_to_image_plane=True,      # Planar depth\n    normals=True,                      # Surface normals\n    motion_vectors=True                # Optical flow\n)\nwriter.attach([camera])\n'})}),"\n",(0,a.jsx)(n.h2,{id:"creating-a-custom-dataset-for-humanoid-perception",children:"Creating a Custom Dataset for Humanoid Perception"}),"\n",(0,a.jsx)(n.p,{children:"This example generates training data for detecting objects a humanoid robot might manipulate:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Define objects humanoid should detect\ndef create_manipulation_scene():\n    # Ground plane with random texture\n    floor = rep.create.plane(\n        scale=10,\n        semantics=[("class", "floor")]\n    )\n    with floor:\n        rep.randomizer.texture(\n            textures=["./textures/wood.jpg", "./textures/tile.jpg"]\n        )\n\n    # Table at humanoid waist height\n    table = rep.create.cube(\n        position=(1.0, 0, 0.75),\n        scale=(1.0, 0.6, 0.05),\n        semantics=[("class", "table")]\n    )\n\n    # Randomized target objects on table\n    obj_types = ["cube", "sphere", "cylinder"]\n    for i in range(3):  # 3 random objects\n        obj = rep.create.from_usd(\n            f"./assets/{rep.distribution.choice(obj_types)}.usd",\n            position=rep.distribution.uniform(\n                (0.5, -0.3, 0.80),  # Table surface bounds\n                (1.5, 0.3, 0.80)\n            ),\n            semantics=[("class", "manipulable_object")]\n        )\n        with obj:\n            # Random materials make model robust to appearance\n            rep.randomizer.color(\n                colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\n            )\n\n    # Humanoid eye-level camera (160cm height)\n    camera = rep.create.camera(\n        position=(0, 0, 1.6),\n        rotation=rep.distribution.uniform((-10, -30, 0), (10, 30, 0))\n    )\n\n    return camera\n\nrep.randomizer.register(create_manipulation_scene)\n\n# Generate dataset with consistent format\nwriter = rep.WriterRegistry.get("BasicWriter")\nwriter.initialize(\n    output_dir="./humanoid_manipulation_dataset",\n    rgb=True,\n    bounding_box_2d_tight=True,\n    semantic_segmentation=True\n)\n\ncamera = create_manipulation_scene()\nwriter.attach([camera])\n\n# Generate 5000 training images\nwith rep.trigger.on_frame(num_frames=5000):\n    rep.randomizer.create_manipulation_scene()\n\nrep.orchestrator.run()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Match Real Sensors"}),": Configure camera resolution, FOV, and noise to match your physical robot's sensors."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sufficient Variation"}),": Generate at least 10x more synthetic data than you would collect manually."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate Transfer"}),": Test trained models on small real-world validation sets to verify sim-to-real gap."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Incremental Complexity"}),": Start with simple scenes, add complexity as models improve."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Basic Randomization"}),": Create a dataset of 100 images with cubes of random colors and sizes at random positions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Lighting Study"}),": Generate 50 images of the same scene with only lighting randomized. Observe how this affects object appearance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Custom Objects"}),": Import a 3D model of an everyday object (cup, bottle) and generate a 1000-image detection dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Ground Truth Comparison"}),": Generate RGB + semantic segmentation pairs and visualize the segmentation masks overlaid on RGB images."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Next"}),": ",(0,a.jsx)(n.a,{href:"/hackathon-book/module-03-isaac/week-9-isaac-ros",children:"Week 9 - Isaac ROS Deployment"})]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);