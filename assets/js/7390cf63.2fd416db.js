"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[7188],{1398:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/ai-13-ed01b228f10026021530f0817a1b229f.png"},3968:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-04-vla/week-12-multimodal","title":"Week 12: Multimodal Vision-Language Fusion","description":"Multimodal Vision-Language Fusion","source":"@site/docs/module-04-vla/week-12-multimodal.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-12-multimodal","permalink":"/hackathon-book/module-04-vla/week-12-multimodal","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-04-vla/week-12-multimodal.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 11: LLM-Based Task Planning","permalink":"/hackathon-book/module-04-vla/week-11-llm-planning"},"next":{"title":"Week 12: Gesture Recognition for Human-Robot Interaction","permalink":"/hackathon-book/module-04-vla/week-12-gesture-recognition"}}');var o=i(4848),s=i(8453);const a={},r="Week 12: Multimodal Vision-Language Fusion",l={},c=[{value:"Vision and Language Integration",id:"vision-and-language-integration",level:2},{value:"Why Multimodal Models?",id:"why-multimodal-models",level:3},{value:"CLIP for Visual Grounding",id:"clip-for-visual-grounding",level:2},{value:"Installation and Basic Usage",id:"installation-and-basic-usage",level:3},{value:"Referring Expressions for Object Selection",id:"referring-expressions-for-object-selection",level:2},{value:"Spatial Grounding with CLIP",id:"spatial-grounding-with-clip",level:3},{value:"Visual Question Answering (VQA)",id:"visual-question-answering-vqa",level:2},{value:"VQA with GPT-4V (Vision)",id:"vqa-with-gpt-4v-vision",level:3},{value:"Integrating Vision-Language with Robot Control",id:"integrating-vision-language-with-robot-control",level:2},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"week-12-multimodal-vision-language-fusion",children:"Week 12: Multimodal Vision-Language Fusion"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Multimodal Vision-Language Fusion",src:i(1398).A+"",width:"576",height:"1024"})}),"\n",(0,o.jsx)(n.h2,{id:"vision-and-language-integration",children:"Vision and Language Integration"}),"\n",(0,o.jsx)(n.p,{children:'Multimodal models jointly process visual and textual data to ground language in physical perception. For robotics, this enables referring to objects by natural descriptions ("the mug with the red handle") rather than pre-defined IDs or bounding box coordinates. Vision-language fusion is essential for embodied AI agents operating in open-world environments with novel, unnamed objects.'}),"\n",(0,o.jsx)(n.h3,{id:"why-multimodal-models",children:"Why Multimodal Models?"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Zero-Shot Object Recognition"}),': Traditional object detectors require training on labeled datasets. Multimodal models leverage language descriptions to recognize new categories without retraining\u2014a humanoid encountering "quinoa" for the first time can identify it via text-image similarity.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Spatial Reasoning"}),': Language models encode spatial relationships ("left of", "behind") but lack visual grounding. Vision-language models learn to map these linguistic concepts to pixel coordinates.']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Disambiguation"}),': In scenes with multiple similar objects ("the red mug on the left"), visual grounding resolves which instance matches the description.']}),"\n",(0,o.jsx)(n.h2,{id:"clip-for-visual-grounding",children:"CLIP for Visual Grounding"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"CLIP (Contrastive Language-Image Pre-training)"})," learns a shared embedding space for images and text by training on 400 million image-caption pairs scraped from the internet. Given an image and multiple text descriptions, CLIP computes similarity scores to determine the best match."]}),"\n",(0,o.jsx)(n.h3,{id:"installation-and-basic-usage",children:"Installation and Basic Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Install CLIP from OpenAI\'s official repository\nimport subprocess\nsubprocess.run(["pip", "install", "git+https://github.com/openai/CLIP.git", "pillow", "torch"])\n\nimport torch\nimport clip\nfrom PIL import Image\nimport numpy as np\n\n# Load pre-trained CLIP model\n# Options: RN50, RN101, ViT-B/32, ViT-L/14 (larger = more accurate but slower)\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n\ndef ground_object_in_image(image_path: str, text_queries: list) -> dict:\n    """\n    Find which text description best matches the image.\n\n    Args:\n        image_path: Path to image file\n        text_queries: List of text descriptions to compare\n\n    Returns:\n        dict: {\n            \'best_match\': str,\n            \'scores\': dict mapping query to similarity score\n        }\n    """\n    # Load and preprocess image\n    image = Image.open(image_path)\n    image_input = preprocess(image).unsqueeze(0).to(device)\n\n    # Tokenize text queries\n    text_inputs = clip.tokenize(text_queries).to(device)\n\n    # Compute embeddings\n    with torch.no_grad():\n        image_features = model.encode_image(image_input)  # Shape: [1, 512]\n        text_features = model.encode_text(text_inputs)    # Shape: [N, 512]\n\n        # Normalize features to unit vectors\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        # Compute cosine similarity (dot product of unit vectors)\n        similarity = (image_features @ text_features.T).squeeze(0)  # Shape: [N]\n\n        # Convert to probabilities using softmax\n        probabilities = similarity.softmax(dim=0).cpu().numpy()\n\n    # Create results dictionary\n    scores = {query: float(prob) for query, prob in zip(text_queries, probabilities)}\n    best_match = text_queries[probabilities.argmax()]\n\n    return {\n        \'best_match\': best_match,\n        \'scores\': scores\n    }\n\n# Example: Object classification in kitchen scene\nqueries = [\n    "a red coffee mug",\n    "a white plate",\n    "a stainless steel fork",\n    "a glass of water"\n]\n\nresult = ground_object_in_image("kitchen_counter.jpg", queries)\nprint(f"Best match: {result[\'best_match\']}")\nprint("\\nAll scores:")\nfor query, score in result[\'scores\'].items():\n    print(f"  {query}: {score:.3f}")\n\n# Output:\n# Best match: a red coffee mug\n# All scores:\n#   a red coffee mug: 0.782\n#   a white plate: 0.124\n#   a stainless steel fork: 0.061\n#   a glass of water: 0.033\n'})}),"\n",(0,o.jsx)(n.h2,{id:"referring-expressions-for-object-selection",children:"Referring Expressions for Object Selection"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Referring expressions"})," uniquely identify objects using attributes (color, shape), spatial relations (left, behind), and functional descriptions (for drinking). Unlike simple object detection, this requires compositional reasoning."]}),"\n",(0,o.jsx)(n.h3,{id:"spatial-grounding-with-clip",children:"Spatial Grounding with CLIP"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import cv2\nfrom typing import List, Tuple\n\ndef detect_objects_with_clip(\n    image_path: str,\n    object_descriptions: List[str],\n    grid_size: int = 8\n) -> List[Tuple[str, int, int, float]]:\n    """\n    Localize objects in image using sliding window CLIP scoring.\n\n    Args:\n        image_path: Input image\n        object_descriptions: List of text descriptions to search for\n        grid_size: Divide image into grid_size x grid_size regions\n\n    Returns:\n        List of (description, x, y, score) for detected objects\n    """\n    # Load image\n    image = Image.open(image_path)\n    img_width, img_height = image.size\n\n    # Calculate region dimensions\n    region_width = img_width // grid_size\n    region_height = img_height // grid_size\n\n    detections = []\n\n    for desc in object_descriptions:\n        best_score = 0\n        best_position = (0, 0)\n\n        # Slide over grid regions\n        for i in range(grid_size):\n            for j in range(grid_size):\n                # Extract region\n                left = j * region_width\n                top = i * region_height\n                right = left + region_width\n                bottom = top + region_height\n\n                region = image.crop((left, top, right, bottom))\n                region_input = preprocess(region).unsqueeze(0).to(device)\n\n                # Compute CLIP score for this region\n                text_input = clip.tokenize([desc]).to(device)\n\n                with torch.no_grad():\n                    image_features = model.encode_image(region_input)\n                    text_features = model.encode_text(text_input)\n\n                    image_features /= image_features.norm(dim=-1, keepdim=True)\n                    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n                    score = (image_features @ text_features.T).item()\n\n                # Track highest scoring region for this object\n                if score > best_score:\n                    best_score = score\n                    best_position = (left + region_width // 2, top + region_height // 2)\n\n        detections.append((desc, best_position[0], best_position[1], best_score))\n\n    return detections\n\n# Example: Locate multiple objects\nobjects = ["red mug", "laptop computer", "potted plant"]\ndetections = detect_objects_with_clip("desk_scene.jpg", objects, grid_size=6)\n\nfor obj, x, y, score in detections:\n    print(f"{obj}: position ({x}, {y}), confidence {score:.3f}")\n\n# Visualize results\nimage = cv2.imread("desk_scene.jpg")\nfor obj, x, y, score in detections:\n    cv2.circle(image, (x, y), 10, (0, 255, 0), -1)\n    cv2.putText(image, obj, (x + 15, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\ncv2.imwrite("detected_objects.jpg", image)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"visual-question-answering-vqa",children:"Visual Question Answering (VQA)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Visual Question Answering"}),' combines image understanding with language comprehension to answer open-ended questions about visual scenes. For humanoid robots, VQA enables situational awareness: "Is the door open?", "How many people are in the room?", "What color is the object on the table?"']}),"\n",(0,o.jsx)(n.h3,{id:"vqa-with-gpt-4v-vision",children:"VQA with GPT-4V (Vision)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import base64\nimport requests\n\ndef answer_visual_question(image_path: str, question: str) -> str:\n    """\n    Use GPT-4V to answer questions about an image.\n\n    Args:\n        image_path: Path to image file\n        question: Natural language question\n\n    Returns:\n        str: Answer from GPT-4V\n    """\n    # Encode image to base64 for API transmission\n    with open(image_path, "rb") as image_file:\n        image_data = base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n    headers = {\n        "Content-Type": "application/json",\n        "Authorization": f"Bearer {os.getenv(\'OPENAI_API_KEY\')}"\n    }\n\n    # Construct multimodal prompt\n    payload = {\n        "model": "gpt-4-vision-preview",\n        "messages": [\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": question\n                    },\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": f"data:image/jpeg;base64,{image_data}"\n                        }\n                    }\n                ]\n            }\n        ],\n        "max_tokens": 300\n    }\n\n    # Send API request\n    response = requests.post(\n        "https://api.openai.com/v1/chat/completions",\n        headers=headers,\n        json=payload\n    )\n\n    # Extract answer\n    result = response.json()\n    answer = result[\'choices\'][0][\'message\'][\'content\']\n    return answer\n\n# Example: Scene understanding queries\nquestions = [\n    "How many mugs are on the table?",\n    "What color is the leftmost mug?",\n    "Is there a laptop visible in the image?",\n    "Describe the spatial arrangement of objects on the desk."\n]\n\nfor q in questions:\n    answer = answer_visual_question("desk_scene.jpg", q)\n    print(f"Q: {q}")\n    print(f"A: {answer}\\n")\n\n# Output:\n# Q: How many mugs are on the table?\n# A: There are three mugs visible on the table.\n#\n# Q: What color is the leftmost mug?\n# A: The leftmost mug is red with a white handle.\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-vision-language-with-robot-control",children:"Integrating Vision-Language with Robot Control"}),"\n",(0,o.jsx)(n.p,{children:"Combine CLIP grounding with ROS 2 action servers for closed-loop manipulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point\n\nclass VisionLanguageGrasping(Node):\n    """\n    ROS 2 node that uses CLIP to locate objects and commands robot to grasp.\n    """\n\n    def __init__(self):\n        super().__init__(\'vision_language_grasping\')\n\n        # Subscribe to camera images\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Publisher for grasp target positions\n        self.grasp_pub = self.create_publisher(Point, \'/grasp_target\', 10)\n\n        self.current_image = None\n\n    def image_callback(self, msg):\n        """Store latest camera image."""\n        # Convert ROS Image message to PIL Image\n        self.current_image = self.ros_to_pil_image(msg)\n\n    def grasp_object_by_description(self, description: str):\n        """\n        Find object matching description and publish grasp target.\n\n        Args:\n            description: Text description (e.g., "the blue screwdriver")\n        """\n        if self.current_image is None:\n            self.get_logger().warn("No camera image available")\n            return\n\n        # Use CLIP to locate object\n        detections = detect_objects_with_clip(\n            self.current_image,\n            [description],\n            grid_size=8\n        )\n\n        if detections:\n            desc, x, y, score = detections[0]\n            self.get_logger().info(\n                f"Found \'{desc}\' at pixel ({x}, {y}) with confidence {score:.2f}"\n            )\n\n            # Convert pixel coordinates to 3D point (requires camera calibration)\n            grasp_point = self.pixel_to_3d(x, y)\n\n            # Publish grasp target for manipulation controller\n            self.grasp_pub.publish(grasp_point)\n\n    def pixel_to_3d(self, x: int, y: int) -> Point:\n        """\n        Convert 2D pixel to 3D point using depth sensor and camera intrinsics.\n        (Simplified; real implementation requires camera_info and depth_image)\n        """\n        point = Point()\n        point.x = float(x) * 0.001  # Mock conversion\n        point.y = float(y) * 0.001\n        point.z = 0.5  # Assume fixed depth\n        return point\n\n# Example usage in ROS 2 context\ndef main():\n    rclpy.init()\n    node = VisionLanguageGrasping()\n\n    # Command robot to grasp object\n    node.grasp_object_by_description("the red coffee mug")\n\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,o.jsx)(n.p,{children:"Build a visual search system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Capture image from robot's camera"}),"\n",(0,o.jsx)(n.li,{children:'User provides text query: "Find the green bottle"'}),"\n",(0,o.jsx)(n.li,{children:"Use CLIP to locate object and compute bounding box"}),"\n",(0,o.jsx)(n.li,{children:"Navigate robot to face object (pan camera or move base)"}),"\n",(0,o.jsx)(n.li,{children:"Verify object is within manipulation workspace"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Extend with ",(0,o.jsx)(n.strong,{children:"attribute reasoning"}),': Handle queries like "the larger of the two mugs" (requires size comparison) or "the mug closest to the edge" (spatial reasoning).']}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["You've connected language to vision for object grounding. In ",(0,o.jsx)(n.strong,{children:"Week 12: Gesture Recognition"}),", you'll add human pose tracking using MediaPipe, enabling robots to interpret pointing gestures and body language for natural human-robot interaction."]})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);