"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[623],{4184:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-04-vla/week-12-gesture-recognition","title":"Week 12: Gesture Recognition for Human-Robot Interaction","description":"Gesture Recognition","source":"@site/docs/module-04-vla/week-12-gesture-recognition.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-12-gesture-recognition","permalink":"/hackathon-book/module-04-vla/week-12-gesture-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/Asmayaseen/hackathon-book/tree/main/frontend/docs/module-04-vla/week-12-gesture-recognition.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 12: Multimodal Vision-Language Fusion","permalink":"/hackathon-book/module-04-vla/week-12-multimodal"},"next":{"title":"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI","permalink":"/hackathon-book/module-04-vla/week-13-capstone-intro"}}');var s=i(4848),r=i(8453);const o={},a="Week 12: Gesture Recognition for Human-Robot Interaction",d={},l=[{value:"Hand Gesture Detection with MediaPipe",id:"hand-gesture-detection-with-mediapipe",level:2},{value:"Why MediaPipe for Robotics?",id:"why-mediapipe-for-robotics",level:3},{value:"Installation and Basic Hand Tracking",id:"installation-and-basic-hand-tracking",level:3},{value:"Gesture Command Mapping",id:"gesture-command-mapping",level:2},{value:"Pose Estimation for Human-Robot Interaction",id:"pose-estimation-for-human-robot-interaction",level:2},{value:"Full-Body Pose Tracking",id:"full-body-pose-tracking",level:3},{value:"Integrating Gestures with ROS 2",id:"integrating-gestures-with-ros-2",level:2},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"week-12-gesture-recognition-for-human-robot-interaction",children:"Week 12: Gesture Recognition for Human-Robot Interaction"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.img,{alt:"Gesture Recognition",src:i(4473).A+"",width:"736",height:"736"})}),"\n",(0,s.jsx)(e.h2,{id:"hand-gesture-detection-with-mediapipe",children:"Hand Gesture Detection with MediaPipe"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"MediaPipe"})," is Google's open-source framework for real-time perception pipelines, offering pre-trained models for hand tracking, pose estimation, face detection, and holistic body tracking. For humanoid robotics, hand gesture recognition enables non-verbal communication\u2014pointing to objects, signaling stop/go, or demonstrating manipulation trajectories."]}),"\n",(0,s.jsx)(e.h3,{id:"why-mediapipe-for-robotics",children:"Why MediaPipe for Robotics?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Real-Time Performance"}),": MediaPipe achieves 30+ FPS on CPU and 60+ FPS on GPU, suitable for responsive human-robot interaction without specialized hardware."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Cross-Platform"}),": Runs on Linux, Windows, mobile devices, and embedded systems (Raspberry Pi, Jetson Nano), enabling deployment on diverse robot platforms."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Pre-Trained Models"}),": MediaPipe's hand tracker identifies 21 3D landmarks per hand (fingertips, knuckles, wrist) without custom training\u2014no dataset annotation required."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multi-Hand Support"}),": Detects and tracks multiple hands simultaneously, distinguishing left/right hand and maintaining identity across frames."]}),"\n",(0,s.jsx)(e.h3,{id:"installation-and-basic-hand-tracking",children:"Installation and Basic Hand Tracking"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Install MediaPipe and OpenCV for visualization\nimport subprocess\nsubprocess.run(["pip", "install", "mediapipe", "opencv-python", "numpy"])\n\nimport cv2\nimport mediapipe as mp\nimport numpy as np\n\n# Initialize MediaPipe hand tracking\nmp_hands = mp.solutions.hands\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\ndef track_hands_in_video(video_source=0):\n    """\n    Real-time hand tracking from webcam or video file.\n\n    Args:\n        video_source: Camera index (0 for default webcam) or video file path\n    """\n    # Configure hand detector\n    # max_num_hands: Maximum hands to detect (1-2 for most robotics tasks)\n    # min_detection_confidence: Threshold for initial hand detection (0.0-1.0)\n    # min_tracking_confidence: Threshold for landmark tracking (0.0-1.0)\n    hands = mp_hands.Hands(\n        static_image_mode=False,  # False for video stream (enables tracking)\n        max_num_hands=2,\n        min_detection_confidence=0.7,  # Higher = fewer false positives\n        min_tracking_confidence=0.5\n    )\n\n    cap = cv2.VideoCapture(video_source)\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n\n        # Convert BGR to RGB (MediaPipe expects RGB)\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Process frame to detect hands\n        results = hands.process(frame_rgb)\n\n        # Draw hand landmarks on frame\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Draw 21 landmarks and connections\n                mp_drawing.draw_landmarks(\n                    frame,\n                    hand_landmarks,\n                    mp_hands.HAND_CONNECTIONS,\n                    mp_drawing_styles.get_default_hand_landmarks_style(),\n                    mp_drawing_styles.get_default_hand_connections_style()\n                )\n\n        # Display annotated frame\n        cv2.imshow(\'Hand Tracking\', frame)\n\n        if cv2.waitKey(5) & 0xFF == ord(\'q\'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n    hands.close()\n\n# Run hand tracker\ntrack_hands_in_video()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"gesture-command-mapping",children:"Gesture Command Mapping"}),"\n",(0,s.jsx)(e.p,{children:"Map hand poses to robot commands by analyzing landmark configurations:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from typing import Optional, Dict\nfrom dataclasses import dataclass\n\n@dataclass\nclass HandGesture:\n    """Represents a recognized gesture with confidence score."""\n    name: str\n    confidence: float\n    parameters: Dict  # Additional info (e.g., pointing direction)\n\nclass GestureRecognizer:\n    """\n    Classify hand gestures from MediaPipe landmarks.\n    Maps gestures to robot commands.\n    """\n\n    def __init__(self):\n        self.hands = mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=1,  # Single hand for command gestures\n            min_detection_confidence=0.7,\n            min_tracking_confidence=0.5\n        )\n\n    def recognize_gesture(self, frame: np.ndarray) -> Optional[HandGesture]:\n        """\n        Detect and classify gesture in image frame.\n\n        Args:\n            frame: BGR image from camera\n\n        Returns:\n            HandGesture object or None if no gesture detected\n        """\n        # Process frame\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = self.hands.process(frame_rgb)\n\n        if not results.multi_hand_landmarks:\n            return None\n\n        # Analyze first detected hand\n        landmarks = results.multi_hand_landmarks[0]\n\n        # Convert landmarks to numpy array for easier computation\n        # Each landmark has x, y, z coordinates (normalized to [0, 1])\n        points = np.array([\n            [lm.x, lm.y, lm.z]\n            for lm in landmarks.landmark\n        ])\n\n        # Classify gesture based on landmark configuration\n        if self._is_thumbs_up(points):\n            return HandGesture("thumbs_up", 0.95, {})\n        elif self._is_pointing(points):\n            direction = self._get_pointing_direction(points)\n            return HandGesture("pointing", 0.90, {"direction": direction})\n        elif self._is_open_palm(points):\n            return HandGesture("stop", 0.92, {})\n        elif self._is_closed_fist(points):\n            return HandGesture("grab", 0.88, {})\n        elif self._is_peace_sign(points):\n            return HandGesture("peace", 0.85, {})\n\n        return HandGesture("unknown", 0.5, {})\n\n    def _is_thumbs_up(self, points: np.ndarray) -> bool:\n        """\n        Detect thumbs-up gesture.\n        Criteria: Thumb tip above MCP joint, other fingers curled.\n        """\n        # Landmark indices (see MediaPipe hand landmark diagram)\n        thumb_tip = points[4]\n        thumb_mcp = points[2]\n        index_tip = points[8]\n        index_pip = points[6]\n\n        # Thumb extended upward\n        thumb_extended = thumb_tip[1] < thumb_mcp[1]  # y decreases upward\n\n        # Other fingers curled (tip below PIP joint)\n        fingers_curled = all([\n            points[8][1] > points[6][1],   # Index\n            points[12][1] > points[10][1],  # Middle\n            points[16][1] > points[14][1],  # Ring\n            points[20][1] > points[18][1]   # Pinky\n        ])\n\n        return thumb_extended and fingers_curled\n\n    def _is_pointing(self, points: np.ndarray) -> bool:\n        """\n        Detect pointing gesture.\n        Criteria: Index finger extended, other fingers curled.\n        """\n        # Index finger extended\n        index_extended = points[8][1] < points[6][1]  # Tip above PIP\n\n        # Other fingers curled\n        other_curled = all([\n            points[12][1] > points[10][1],  # Middle\n            points[16][1] > points[14][1],  # Ring\n            points[20][1] > points[18][1]   # Pinky\n        ])\n\n        return index_extended and other_curled\n\n    def _is_open_palm(self, points: np.ndarray) -> bool:\n        """\n        Detect open palm (stop signal).\n        Criteria: All fingers extended and spread.\n        """\n        # All fingertips above their respective MCP joints\n        fingers_extended = all([\n            points[4][1] < points[2][1],    # Thumb\n            points[8][1] < points[5][1],    # Index\n            points[12][1] < points[9][1],   # Middle\n            points[16][1] < points[13][1],  # Ring\n            points[20][1] < points[17][1]   # Pinky\n        ])\n\n        return fingers_extended\n\n    def _is_closed_fist(self, points: np.ndarray) -> bool:\n        """\n        Detect closed fist (grab command).\n        Criteria: All fingers curled below MCP joints.\n        """\n        all_curled = all([\n            points[8][1] > points[5][1],    # Index\n            points[12][1] > points[9][1],   # Middle\n            points[16][1] > points[13][1],  # Ring\n            points[20][1] > points[17][1]   # Pinky\n        ])\n\n        return all_curled\n\n    def _is_peace_sign(self, points: np.ndarray) -> bool:\n        """\n        Detect peace sign (index + middle extended).\n        """\n        index_extended = points[8][1] < points[6][1]\n        middle_extended = points[12][1] < points[10][1]\n        ring_curled = points[16][1] > points[14][1]\n        pinky_curled = points[20][1] > points[18][1]\n\n        return index_extended and middle_extended and ring_curled and pinky_curled\n\n    def _get_pointing_direction(self, points: np.ndarray) -> str:\n        """\n        Compute pointing direction from index finger orientation.\n\n        Returns:\n            Direction string: "left", "right", "up", "down", "forward"\n        """\n        # Vector from index MCP to tip\n        mcp = points[5]\n        tip = points[8]\n        vector = tip - mcp\n\n        # Analyze primary component\n        if abs(vector[0]) > abs(vector[1]):\n            return "right" if vector[0] > 0 else "left"\n        else:\n            return "down" if vector[1] > 0 else "up"\n\n# Example: Real-time gesture recognition\nrecognizer = GestureRecognizer()\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n\n    gesture = recognizer.recognize_gesture(frame)\n\n    if gesture and gesture.confidence > 0.8:\n        # Display recognized gesture\n        text = f"{gesture.name} ({gesture.confidence:.2f})"\n        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                    1, (0, 255, 0), 2)\n\n        # Execute robot command based on gesture\n        if gesture.name == "pointing":\n            print(f"Robot: Navigate {gesture.parameters[\'direction\']}")\n        elif gesture.name == "grab":\n            print("Robot: Execute grasp")\n        elif gesture.name == "stop":\n            print("Robot: Emergency stop")\n\n    cv2.imshow(\'Gesture Recognition\', frame)\n    if cv2.waitKey(5) & 0xFF == ord(\'q\'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"pose-estimation-for-human-robot-interaction",children:"Pose Estimation for Human-Robot Interaction"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Pose estimation"})," tracks full-body landmarks (33 points including shoulders, elbows, hips, knees) to understand human activity and spatial positioning. For collaborative robotics, this enables:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Proximity Detection"}),": Stop robot arm if human enters workspace"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Activity Recognition"}),": Distinguish standing, sitting, walking for context-aware behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Gesture Demonstration"}),": Learn manipulation tasks by imitating human arm movements"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"full-body-pose-tracking",children:"Full-Body Pose Tracking"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'mp_pose = mp.solutions.pose\n\ndef track_human_pose(video_source=0):\n    """\n    Track human body pose for safety monitoring and imitation learning.\n\n    Args:\n        video_source: Camera index or video file\n    """\n    pose = mp_pose.Pose(\n        static_image_mode=False,\n        model_complexity=1,  # 0=lite, 1=full, 2=heavy (higher = more accurate)\n        smooth_landmarks=True,  # Temporal smoothing for video\n        min_detection_confidence=0.5,\n        min_tracking_confidence=0.5\n    )\n\n    cap = cv2.VideoCapture(video_source)\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = pose.process(frame_rgb)\n\n        if results.pose_landmarks:\n            # Draw pose skeleton\n            mp_drawing.draw_landmarks(\n                frame,\n                results.pose_landmarks,\n                mp_pose.POSE_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n            )\n\n            # Extract key joint positions for analysis\n            landmarks = results.pose_landmarks.landmark\n            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\n\n            # Example: Detect if hands are raised (y < 0.5)\n            hands_raised = (left_wrist.y < 0.5 and right_wrist.y < 0.5)\n\n            if hands_raised:\n                cv2.putText(frame, "Hands Raised - Robot Paused", (10, 60),\n                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n        cv2.imshow(\'Pose Tracking\', frame)\n        if cv2.waitKey(5) & 0xFF == ord(\'q\'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n    pose.close()\n\n# Run pose tracker\ntrack_human_pose()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"integrating-gestures-with-ros-2",children:"Integrating Gestures with ROS 2"}),"\n",(0,s.jsx)(e.p,{children:"Connect gesture recognition to robot control:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass GestureControlNode(Node):\n    """\n    ROS 2 node that publishes robot commands based on hand gestures.\n    """\n\n    def __init__(self):\n        super().__init__(\'gesture_control\')\n\n        # Publisher for gesture-based commands\n        self.cmd_pub = self.create_publisher(String, \'/gesture_commands\', 10)\n\n        # Timer for periodic gesture recognition\n        self.timer = self.create_timer(0.1, self.timer_callback)  # 10 Hz\n\n        self.recognizer = GestureRecognizer()\n        self.cap = cv2.VideoCapture(0)\n\n        self.last_gesture = None\n        self.gesture_stable_count = 0\n        self.STABILITY_THRESHOLD = 5  # Require 5 consecutive frames for confirmation\n\n    def timer_callback(self):\n        """Process camera frame and publish gesture commands."""\n        success, frame = self.cap.read()\n        if not success:\n            return\n\n        gesture = self.recognizer.recognize_gesture(frame)\n\n        if gesture and gesture.confidence > 0.85:\n            # Require gesture stability to avoid false triggers\n            if gesture.name == self.last_gesture:\n                self.gesture_stable_count += 1\n            else:\n                self.gesture_stable_count = 0\n                self.last_gesture = gesture.name\n\n            # Publish command if gesture is stable\n            if self.gesture_stable_count >= self.STABILITY_THRESHOLD:\n                msg = String()\n                msg.data = f"{gesture.name}:{gesture.parameters}"\n                self.cmd_pub.publish(msg)\n\n                self.get_logger().info(f"Published gesture command: {gesture.name}")\n\n                # Reset to avoid repeated commands\n                self.gesture_stable_count = 0\n\ndef main():\n    rclpy.init()\n    node = GestureControlNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,s.jsx)(e.p,{children:"Build a gesture-controlled navigation system:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Implement pointing direction detection (8 directions: N, NE, E, SE, S, SW, W, NW)"}),"\n",(0,s.jsxs)(e.li,{children:["Map gestures to navigation commands:","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Point \u2192 Navigate in indicated direction"}),"\n",(0,s.jsx)(e.li,{children:"Open palm \u2192 Stop"}),"\n",(0,s.jsx)(e.li,{children:"Closed fist \u2192 Return to home position"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.li,{children:"Add gesture confirmation: Require 1-second hold before executing command"}),"\n",(0,s.jsx)(e.li,{children:"Implement safety checks: Stop robot if human enters proximity (use pose estimation)"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(e.p,{children:["You've completed multimodal perception (voice, vision, gestures). In ",(0,s.jsx)(e.strong,{children:"Week 13: Capstone Project"}),", you'll integrate all components into an autonomous humanoid system with conversational AI, visual grounding, and adaptive task execution."]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},4473:(n,e,i)=>{i.d(e,{A:()=>t});const t=i.p+"assets/images/ai-14-4affdfa303c322bf150f9819dc34a14e.png"},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);